{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/dgershm1/hympi-ml-retrieval/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | output_path | Sequential | 2.4 K  | train\n",
      "1 | loss        | MSELoss    | 0      | train\n",
      "---------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1182/1182 [01:45<00:00, 11.17it/s, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1182/1182 [01:46<00:00, 11.06it/s, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import keras\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from hympi_ml.data import DataSpec, AMPRSpec, NRSpec\n",
    "from hympi_ml.data.ch06 import Ch06Source\n",
    "from hympi_ml.data.model_dataset import ModelDataset\n",
    "\n",
    "\n",
    "class SpecModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: dict[str, DataSpec],\n",
    "        targets: dict[str, DataSpec],\n",
    "        feature_paths: dict[str, nn.Module],\n",
    "        output_path: nn.Module,\n",
    "        loss,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.feature_paths = feature_paths\n",
    "        self.output_path = output_path\n",
    "        self.loss = loss\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features_forward = [path(inputs[k]) for k, path in self.feature_paths.items()]\n",
    "        return self.output_path(torch.concat(features_forward))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        return nn.functional.mse_loss(output, target[\"TEMPERATURE\"])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "features={\"AMPR\": AMPRSpec()}\n",
    "targets={\"TEMPERATURE\": NRSpec(dataset=\"TEMPERATURE\")}\n",
    "\n",
    "model = SpecModel(\n",
    "    features=features,\n",
    "    targets=targets,\n",
    "    feature_paths={\n",
    "        \"AMPR\": nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "        )\n",
    "    },\n",
    "    output_path=nn.Sequential(\n",
    "        nn.Linear(32, 72),\n",
    "    ),\n",
    "    loss=nn.MSELoss(),\n",
    ")\n",
    "\n",
    "dataset = ModelDataset(Ch06Source(days=[\"20060115\"]), features=features, targets=targets, batch_size=16384)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=None, shuffle=True, num_workers=10)\n",
    "\n",
    "trainer = L.Trainer(enable_progress_bar=True, max_epochs=1, enable_model_summary=True)\n",
    "trainer.fit(model=model, train_dataloaders=loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ceb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
