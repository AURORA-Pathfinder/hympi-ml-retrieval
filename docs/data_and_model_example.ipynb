{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb50c46",
   "metadata": {},
   "source": [
    "# Data and Model Example\n",
    "An explainer and example of how to define the data that goes into your model for training, evaluation, etc.\n",
    "Then, defining the model itself!\n",
    "\n",
    "Since this is a notebook, if you are on VSCode or another IDE, you can hover over each of the class instances / references and see more details or go to their code definitions for further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b48a369",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9faf433",
   "metadata": {},
   "source": [
    "## Defining Model Inputs and Outputs\n",
    "To start, we'll begin with the `ModelDataSpec`, considered a top-level object for defining the specification for your model inputs, outputs, etc.\n",
    "\n",
    "A `ModelDataSpec` is defined by three dictionaries: features, targets, and extras. Each of them are dictionaries that map strings to `DataSpec`.\n",
    "More details on `DataSpec` is provided in the `docs/spec_system_explainer.md`. The features are the inputs to the model, the targets are the outputs. Extras is optionally and doesn't get used during model training at all and can be used for evaluation and pairing features and targets with other data not related to the model itself. Consider adding latitude and longitude specifications for evaluating model performance on specific locations or for filtering out specific locations.\n",
    "\n",
    "Below is an example of a `ModelDataSpec` for a model trained on CoSMIR-H data to predict temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52da784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hympi_ml.data import ModelDataSpec, cosmirh, CosmirhSpec, NRSpec\n",
    "from hympi_ml.data.scale import MinMaxScaler\n",
    "\n",
    "spec = ModelDataSpec(\n",
    "    features={\n",
    "        \"CH\": CosmirhSpec(\n",
    "            frequencies=[\n",
    "                cosmirh.C50_BAND,\n",
    "                cosmirh.C183_BAND,\n",
    "            ],\n",
    "            ignore_frequencies=[  # problematic CRTM frequencies\n",
    "                56.96679675,\n",
    "                57.60742175,\n",
    "                57.611328,\n",
    "                57.61523425,\n",
    "            ],\n",
    "        ),\n",
    "    },\n",
    "    targets={\n",
    "        \"TEMPERATURE\": NRSpec(\n",
    "            dataset=\"TEMPERATURE\",\n",
    "            scaler=MinMaxScaler(minimum=175.0, maximum=325.0),\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f997eb1",
   "metadata": {},
   "source": [
    "As you can see, the above code defines a `ModelDataSpec` with a features (inputs) dictionary that contains a single entry for named \"CH\" for a `CosmirhSpec` instance. The inner workings of that spec don't matter too much for this notebook but feel free to explore the CosmirhSpec code itself for more details.\n",
    "\n",
    "We can also see the targets defined with a single \"TEMPERATURE\" entry that defines a nature run specification `NRSpec` that pulls \"TEMPERATURE\" data. It also scales the data using a min max scaler from 175K to 325K. For more details on `NRSpec` please refer to it's code definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390469d",
   "metadata": {},
   "source": [
    "With our data now defined, we must define where we get our data! This is where `RawDataModule` comes in!\n",
    "This will allow us to use our above spec with multiple `DataSource` definitons for our train, validation, and test datasets.\n",
    "\n",
    "Note: A `RawDataModule` inherits from a PyTorch Lightning `LightningDataModule` which informs its functionality. Visit the docs [here](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) for reference.\n",
    "\n",
    "Below is an example instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29678f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hympi_ml.data import RawDataModule\n",
    "from hympi_ml.data.ch06 import Ch06Source\n",
    "\n",
    "datamodule = RawDataModule(\n",
    "    spec=spec,\n",
    "    train_source=Ch06Source(\n",
    "        days=[\n",
    "            \"20060115\",\n",
    "            \"20060215\",\n",
    "            \"20060415\",\n",
    "            \"20060515\",\n",
    "            \"20060615\",\n",
    "            \"20060715\",\n",
    "            \"20060815\",\n",
    "            \"20061015\",\n",
    "        ]\n",
    "    ),\n",
    "    val_source=Ch06Source(days=[\"20061115\"]),\n",
    "    test_source=Ch06Source(days=[\"20061215\"]),\n",
    "    batch_size=8192,\n",
    "    num_workers=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f22ee9",
   "metadata": {},
   "source": [
    "In the above code, we define a `RawDataModule` which references the `ModelDataSpec` we defined earlier along with three data sources for our train, validation, and test datasets. In this case, we have a different set of days for each as we are using the `CH06Source` based on our common means of splitting up data using days. `RawDataModule` also requires that we define a batch size that our data will load in and the number of workers (or processes) that will be used during loading (20 is a good number for ADAPT but YMMV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa6840",
   "metadata": {},
   "source": [
    "## Defining the Model Itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5aa3b",
   "metadata": {},
   "source": [
    "Now that we have our data, we need a model to work with!\n",
    "\n",
    "But before we define our model, we'll need some metrics that our model will use for training and for our further analysis.\n",
    "\n",
    "For our case, a set of metrics is defined as dictionaries of `MetricCollection`, which is simply another list of metrics. All of our metric functions and classes are from the package `torchmetrics` (more details [here](https://lightning.ai/docs/torchmetrics/stable/)). \n",
    "\n",
    "Below we'll define our metrics for each of our train, validation, and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853520d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import MetricCollection\n",
    "import torchmetrics.regression as re\n",
    "\n",
    "train_metrics = {\n",
    "    \"TEMPERATURE\": MetricCollection(\n",
    "        {\n",
    "            \"mae\": re.MeanAbsoluteError(),\n",
    "        },\n",
    "    ),\n",
    "}\n",
    "\n",
    "val_metrics = {\n",
    "    \"TEMPERATURE\": MetricCollection(\n",
    "        {\n",
    "            \"mae\": re.MeanAbsoluteError(),\n",
    "            \"mse\": re.MeanSquaredError(),\n",
    "            \"rmse\": re.NormalizedRootMeanSquaredError(),\n",
    "        },\n",
    "    ),\n",
    "}\n",
    "\n",
    "test_metrics = {\n",
    "    \"TEMPERATURE\": MetricCollection(\n",
    "        {\n",
    "            \"mae\": re.MeanAbsoluteError(),\n",
    "            \"mse\": re.MeanSquaredError(),\n",
    "            \"rmse\": re.NormalizedRootMeanSquaredError(),\n",
    "        },\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e3b8e",
   "metadata": {},
   "source": [
    "Now that we've defined our metrics, we'll finally get to create our model!\n",
    "\n",
    "In this case it all begins with the idea of a `SpecModel`, which you should consider as a base class for defining any kind of model. It inherits from `LightningModule` from PyTorch Lightning (more details on this module [here](https://lightning.ai/docs/pytorch/stable/model/train_model_basic.html)).\n",
    "\n",
    "Now, for this example we'll use the `MLPModel` which stands for Multi-Layer Perceptron, one of the many options for creating a model. More details about an MLP model type can be found in the introduction section (before any code) [here](https://www.geeksforgeeks.org/deep-learning/multi-layer-perceptron-learning-in-tensorflow/#).\n",
    "\n",
    "Below, we'll define our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from hympi_ml.model import MLPModel\n",
    "\n",
    "model = MLPModel(\n",
    "    spec=spec,\n",
    "    train_metrics=train_metrics,\n",
    "    val_metrics=val_metrics,\n",
    "    test_metrics=test_metrics,\n",
    "    feature_paths=nn.ModuleDict(\n",
    "        {\n",
    "            \"CH\": nn.Sequential(\n",
    "                nn.LazyLinear(1024),\n",
    "                nn.GELU(),\n",
    "                nn.LazyLinear(256),\n",
    "                nn.GELU(),\n",
    "                nn.LazyLinear(128),\n",
    "                nn.GELU(),\n",
    "            ),\n",
    "        }\n",
    "    ),\n",
    "    output_path=nn.Sequential(\n",
    "        nn.LazyLinear(128),\n",
    "        nn.GELU(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed409f6",
   "metadata": {},
   "source": [
    "As you can see, the specification we defined earlier is referenced here as well as all of our metrics. Beyond that, the MLP-specific details are defined as the \"feature paths\" which is a `nn.ModuleDict` for each of our features defined in the spec. The \"output path\" is another PyTorch module that is a single path of layers that is used after all feature paths are concatenated together! An example of this kind of architecture is described in my poster found [here](https://ntrs.nasa.gov/citations/20250003177).\n",
    "\n",
    "After your data, metrics, and model have all been defined, you can now train! This can be done in countless ways, please reference the ends of the files in the `runs` directory. These will have great examples of what to do after everything above has been set up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hympi-ml-retrieval (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
