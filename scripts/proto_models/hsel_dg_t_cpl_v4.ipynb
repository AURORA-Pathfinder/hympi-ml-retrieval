{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "008ce633-8ecc-4d52-a254-a0784d743198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 19:30:55.279754: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-29 19:30:55.392818: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import isfile\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "#os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L1, L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3ea1be-bfd5-4112-989a-8d9bd16bcb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///data/nature_run/work/src/mlruns/394114635930480291', creation_time=1727650112205, experiment_id='394114635930480291', last_update_time=1727650112205, lifecycle_stage='active', name='HSEL+CPL T', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.tensorflow.autolog()\n",
    "mlflow.set_experiment(\"HSEL+CPL T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b48a92-7531-44b6-8461-5c367878d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filelist):\n",
    "    hsel_val = []\n",
    "    scalar_val = []\n",
    "    table_val = []\n",
    "    cpl_val = []\n",
    "\n",
    "    for i in filelist:\n",
    "        # Keys: 'mh', 'hsel', 'scalar', 'table']\n",
    "        tmp = np.load(i)\n",
    "        hsel_val.append(tmp[\"hsel\"])\n",
    "        scalar_val.append(tmp[\"scalar\"])\n",
    "        table_val.append(tmp[\"table\"])\n",
    "        cpl_val.append(tmp[\"cpl\"])\n",
    "\n",
    "    return np.vstack(hsel_val), np.vstack(cpl_val), np.vstack(scalar_val), np.vstack(table_val)\n",
    "\n",
    "\n",
    "train = ['/data/nature_run/fulldays_reduced/all_cpl_20060815.npz',\n",
    "         '/data/nature_run/fulldays_reduced/all_cpl_20060915.npz',\n",
    "         '/data/nature_run/fulldays_reduced/all_cpl_20061015.npz',\n",
    "         '/data/nature_run/fulldays_reduced/all_cpl_20060515.npz',\n",
    "         '/data/nature_run/fulldays_reduced/all_cpl_20060715.npz',\n",
    "         '/data/nature_run/fulldays_reduced/all_cpl_20061115.npz',\n",
    "         '/data/nature_run/fulldays_reduced/all_cpl_20060315.npz', \n",
    "         '/data/nature_run/fulldays_reduced/all_cpl_20061215.npz',\n",
    "         '/data/nature_run/fulldays_reduced/all_cpl_20060615.npz']\n",
    "\n",
    "test = ['/data/nature_run/fulldays_reduced/all_cpl_20060803.npz']\n",
    "\n",
    "validation = ['/data/nature_run/fulldays_reduced/all_cpl_20060803.npz']\n",
    "\n",
    "hsel_train, cpl_train, scalar_train, table_train = get_data(train)\n",
    "hsel_test, cpl_test, scalar_test, table_test = get_data(test)\n",
    "hsel_val, cpl_val, scalar_val, table_val = get_data(validation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c5ebff-6c25-4ae4-a936-6e372a7a3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hsel_train, table_train, scalar_train, hsel_test, table_test, scalar_test = dataload(\"mh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e5dbb2-4a9a-419a-baf7-be1ced137464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hsel_train, table_train, scalar_train, hsel_test, table_test, scalar_test = dataload(\"mh\")\n",
    "#hsel_val = hsel_test.copy()\n",
    "#table_val = table_test.copy()\n",
    "#scalar_val = scalar_test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e622c75-32d0-47dd-8af7-7d25a1d78f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8414706, 1957)\n",
      "(8414706, 733)\n",
      "(8414706, 72, 3)\n",
      "(8414706, 44)\n"
     ]
    }
   ],
   "source": [
    "print(hsel_train.shape)\n",
    "print(cpl_train.shape)\n",
    "print(table_train.shape)\n",
    "print(scalar_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2751b636-5210-4f12-8989-d3f9b0a29eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 20:41:23.961126: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-29 20:41:24.541657: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-09-29 20:41:24.542800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31017 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3e:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "# What should we do\n",
    "# 1) Only use train scalars\n",
    "# 2) Scale seperateley\n",
    "\n",
    "\n",
    "# Presssure ####\n",
    "spress_test = scalar_test[:, 3].reshape(-1, 1)\n",
    "spress_train = scalar_train[:, 3].reshape(-1, 1)\n",
    "spress_val = scalar_val[:, 3].reshape(-1, 1)\n",
    "\n",
    "mins = np.min(spress_train, axis=0)\n",
    "maxs = np.max(spress_train, axis=0)\n",
    "np.savez(\"spress_scalar_cpl_hsel.npz\", mins=mins, maxs=maxs)\n",
    "\n",
    "spress_test = (spress_test - mins)/(maxs - mins)\n",
    "spress_train = (spress_train - mins)/(maxs - mins)\n",
    "spress_val = (spress_val - mins)/(maxs - mins)\n",
    "\n",
    "\n",
    "# Labels ####\n",
    "q_train = table_train[:, :, 1]\n",
    "q_test = table_test[:, :, 1]\n",
    "q_val = table_val[:, :, 1]\n",
    "\n",
    "# Spectra ####\n",
    "mins = np.min(hsel_train, axis=0)\n",
    "maxs = np.max(hsel_train, axis=0)\n",
    "\n",
    "np.savez(\"minimac_scaling_factors_cpl_hsel.npz\", maxs=maxs, mins=mins)\n",
    "\n",
    "hsel_train = np.nan_to_num(hsel_train)\n",
    "hsel_test = np.nan_to_num(hsel_test)\n",
    "hsel_val = np.nan_to_num(hsel_val)\n",
    "\n",
    "hsel_train = (hsel_train - mins)/(maxs - mins)\n",
    "hsel_test = (hsel_test - mins)/(maxs - mins)\n",
    "hsel_val = (hsel_val - mins)/(maxs - mins)\n",
    "\n",
    "# CPL #####\n",
    "mins = np.min(cpl_train, axis=0)\n",
    "maxs = np.max(cpl_train, axis=0)\n",
    "np.savez(\"minimac_scaling_factors_cpl_cpl_hsel.npz\", maxs=maxs, mins=mins)\n",
    "\n",
    "cpl_train = (cpl_train - mins)/(maxs - mins)\n",
    "cpl_test = (cpl_test - mins)/(maxs - mins)\n",
    "cpl_val = (cpl_val - mins)/(maxs - mins)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    # train\n",
    "    x_train = {'rad': tf.convert_to_tensor(hsel_train, np.float32),\n",
    "               'spress': tf.convert_to_tensor(spress_train, np.float32),\n",
    "               'cpl': tf.convert_to_tensor(cpl_train, np.float32)}\n",
    "    del hsel_train\n",
    "    del spress_train\n",
    "    \n",
    "    y_train =  tf.convert_to_tensor(q_train, np.float32)\n",
    "    del q_train\n",
    "\n",
    "    \n",
    "    # Val\n",
    "    x_val = {'rad': tf.convert_to_tensor(hsel_val, np.float32),\n",
    "             'spress': tf.convert_to_tensor(spress_val, np.float32),\n",
    "             'cpl': tf.convert_to_tensor(cpl_val, np.float32)}\n",
    "    del hsel_val\n",
    "    del spress_val\n",
    "    \n",
    "    y_val =  tf.convert_to_tensor(q_val, np.float32)\n",
    "    del q_val\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    x_test = {'rad': tf.convert_to_tensor(hsel_test, np.float32),\n",
    "              'spress': tf.convert_to_tensor(spress_test, np.float32),\n",
    "              'cpl': tf.convert_to_tensor(cpl_test, np.float32)}\n",
    "    del hsel_test\n",
    "    del spress_test\n",
    "    \n",
    "    y_test =  tf.convert_to_tensor(q_test, np.float32)\n",
    "    del q_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66848f44-450d-4748-b044-1d8a1da680a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_test)\n",
    "# print(x_val)\n",
    "# print(y_train.shape)\n",
    "#a = np.argwhere(np.isnan(x_train['rad']))\n",
    "#print(np.sum(np.isnan(x_train['rad'])))\n",
    "#np.nan_to_num(x_train['rad'], copy=False)\n",
    "#print(np.sum(np.isnan(x_train['rad'])))\n",
    "#print(a)\n",
    "#print(len(a))\n",
    "\n",
    "np.sum(np.isnan(x_train['rad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a9a6be-9d65-4f83-b9df-062e3c756f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"Temp\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " rad (InputLayer)               [(None, 1957)]       0           []                               \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 64)           275468      ['rad[0][0]']                    \n",
      "                                                                                                  \n",
      " spress (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 65)           0           ['model_1[0][0]',                \n",
      "                                                                  'spress[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           4224        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           4160        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           4160        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " Temp (Dense)                   (None, 72)           4680        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 292,692\n",
      "Trainable params: 17,224\n",
      "Non-trainable params: 275,468\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_ae_model(config):\n",
    "    # ATMS 22\n",
    "    mh = Input(shape=(config[\"shape\"],), name=\"rad\")\n",
    "    spress = Input(shape=(1,), name=\"spress\")\n",
    "    #cpl = Input(shape=(733,), name=\"cpl\")\n",
    "    # [ha, hb, hc, hd, hw, mh]\n",
    "\n",
    "    encoder = load_model(\"encoder_3_mae.keras\")\n",
    "    encoder.trainable = False\n",
    "\n",
    "    #cpl_reduced = Dense(128, activation=config[\"activation\"])(cpl)\n",
    "    #cpl_reduced = Dense(32, activation=config[\"activation\"])(cpl_reduced)\n",
    "    \n",
    "    enc = encoder(mh)\n",
    "    x = Concatenate()([enc, spress])\n",
    "    for i in range(config[\"num_layers\"]):\n",
    "        x =  Dropout(config[\"dropout\"])(Dense(config[\"num_neurons\"], \n",
    "                                             activation=config[\"activation\"])(x))\n",
    "    outputs = Dense(config['output'], name=\"Temp\")(x)\n",
    "\n",
    "    model = Model(inputs=[mh, spress], outputs=outputs, name=\"Temp\")\n",
    "    model.compile(optimizer=\"adam\", loss='mae')\n",
    "\n",
    "    return model\n",
    "\n",
    "config = {'shape': 1957,\n",
    "          'output': 72,\n",
    "          'dropout': np.random.choice([0.01, 0.1, 0.2, 0.5]),\n",
    "          'num_layers': np.random.choice([3, 4, 5, 10]),\n",
    "          'num_neurons': np.random.choice([16, 32, 64, 128]),\n",
    "          'activation': np.random.choice(['gelu', 'gelu', 'relu'])}\n",
    "\n",
    "model = build_ae_model(config)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137680e3-1dc6-4550-8561-a37c87676066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"Temp\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " rad (InputLayer)               [(None, 1957)]       0           []                               \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 64)           275468      ['rad[0][0]']                    \n",
      "                                                                                                  \n",
      " spress (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 65)           0           ['model_1[0][0]',                \n",
      "                                                                  'spress[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 16)           1056        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 16)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 16)           272         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 16)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 16)           272         ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 16)           0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 16)           272         ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 16)           0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 16)           272         ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 16)           0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " Temp (Dense)                   (None, 72)           1224        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 278,836\n",
      "Trainable params: 3,368\n",
      "Non-trainable params: 275,468\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_ae_cpl_model(config):\n",
    "    # ATMS 22\n",
    "    mh = Input(shape=(config[\"shape\"],), name=\"rad\")\n",
    "    spress = Input(shape=(1,), name=\"spress\")\n",
    "    cpl = Input(shape=(733,), name=\"cpl\")\n",
    "    # [ha, hb, hc, hd, hw, mh]\n",
    "\n",
    "    encoder = load_model(\"encoder_3_mae.keras\")\n",
    "    encoder.trainable = False\n",
    "\n",
    "    cpl_reduced = Dense(128, activation=config[\"activation\"])(cpl)\n",
    "    cpl_reduced = Dense(32, activation=config[\"activation\"])(cpl_reduced)\n",
    "    \n",
    "    enc = encoder(mh)\n",
    "    x = Concatenate()([enc, spress, cpl_reduced])\n",
    "    for i in range(config[\"num_layers\"]):\n",
    "        x =  Dropout(config[\"dropout\"])(Dense(config[\"num_neurons\"], \n",
    "                                             activation=config[\"activation\"])(x))\n",
    "    outputs = Dense(config['output'], name=\"Temp\")(x)\n",
    "\n",
    "    model = Model(inputs=[mh, spress, cpl], outputs=outputs, name=\"Temp\")\n",
    "    model.compile(optimizer=\"adam\", loss='mae')\n",
    "\n",
    "    return model\n",
    "\n",
    "config = {'shape': 1957,\n",
    "          'output': 72,\n",
    "          'dropout': np.random.choice([0.01, 0.1, 0.2, 0.5]),\n",
    "          'num_layers': np.random.choice([3, 4, 5, 10]),\n",
    "          'num_neurons': np.random.choice([16, 32, 64, 128]),\n",
    "          'activation': np.random.choice(['gelu', 'gelu', 'relu'])}\n",
    "\n",
    "model = build_ae_model(config)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3081d9a-e65b-44d5-9c98-9558667c04ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DING\n"
     ]
    }
   ],
   "source": [
    "print(\"DING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b849f517-93eb-4370-bff5-0d7e79fc93cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/29 21:15:04 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/29 21:15:04 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4208/4208 - 71s - loss: 17.0987 - val_loss: 3.8035 - 71s/epoch - 17ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 67s - loss: 10.7509 - val_loss: 2.8777 - 67s/epoch - 16ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 67s - loss: 9.2868 - val_loss: 2.5219 - 67s/epoch - 16ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 67s - loss: 8.9797 - val_loss: 2.8552 - 67s/epoch - 16ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 67s - loss: 8.8654 - val_loss: 2.5955 - 67s/epoch - 16ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 67s - loss: 8.7610 - val_loss: 2.5380 - 67s/epoch - 16ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 66s - loss: 8.6600 - val_loss: 2.5675 - 66s/epoch - 16ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 66s - loss: 8.5505 - val_loss: 2.4078 - 66s/epoch - 16ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 68s - loss: 8.4273 - val_loss: 2.6757 - 68s/epoch - 16ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 67s - loss: 8.3239 - val_loss: 2.5347 - 67s/epoch - 16ms/step\n",
      "Epoch 11/1000\n",
      "4208/4208 - 66s - loss: 8.2187 - val_loss: 2.6594 - 66s/epoch - 16ms/step\n",
      "Epoch 12/1000\n",
      "4208/4208 - 67s - loss: 8.1143 - val_loss: 2.4899 - 67s/epoch - 16ms/step\n",
      "Epoch 13/1000\n",
      "4208/4208 - 68s - loss: 8.0126 - val_loss: 2.5579 - 68s/epoch - 16ms/step\n",
      "Epoch 14/1000\n",
      "4208/4208 - 66s - loss: 7.9104 - val_loss: 2.4825 - 66s/epoch - 16ms/step\n",
      "Epoch 15/1000\n",
      "4208/4208 - 67s - loss: 7.8044 - val_loss: 2.5793 - 67s/epoch - 16ms/step\n",
      "Epoch 16/1000\n",
      "4208/4208 - 66s - loss: 7.7014 - val_loss: 2.4711 - 66s/epoch - 16ms/step\n",
      "1/1 [==============================] - 0s 242ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp4uifao_v/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/29 21:34:07 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/data/jmackin1/miniconda/envs/tf-gpu/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/29 21:39:08 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/29 21:39:08 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4208/4208 - 74s - loss: 28.7892 - val_loss: 4.5848 - 74s/epoch - 18ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 73s - loss: 18.0973 - val_loss: 3.9696 - 73s/epoch - 17ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 73s - loss: 17.4812 - val_loss: 3.6570 - 73s/epoch - 17ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 71s - loss: 17.0973 - val_loss: 3.6564 - 71s/epoch - 17ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 72s - loss: 16.7762 - val_loss: 3.2790 - 72s/epoch - 17ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 71s - loss: 16.5026 - val_loss: 3.3719 - 71s/epoch - 17ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 72s - loss: 16.2153 - val_loss: 3.6873 - 72s/epoch - 17ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 72s - loss: 15.9383 - val_loss: 3.8297 - 72s/epoch - 17ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 73s - loss: 15.6741 - val_loss: 3.1847 - 73s/epoch - 17ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 72s - loss: 15.3892 - val_loss: 3.1500 - 72s/epoch - 17ms/step\n",
      "Epoch 11/1000\n",
      "4208/4208 - 73s - loss: 15.1308 - val_loss: 3.3478 - 73s/epoch - 17ms/step\n",
      "Epoch 12/1000\n",
      "4208/4208 - 73s - loss: 14.8568 - val_loss: 3.2736 - 73s/epoch - 17ms/step\n",
      "Epoch 13/1000\n",
      "4208/4208 - 72s - loss: 14.5813 - val_loss: 3.3119 - 72s/epoch - 17ms/step\n",
      "Epoch 14/1000\n",
      "4208/4208 - 73s - loss: 14.3098 - val_loss: 3.2659 - 73s/epoch - 17ms/step\n",
      "Epoch 15/1000\n",
      "4208/4208 - 73s - loss: 14.0419 - val_loss: 3.4021 - 73s/epoch - 17ms/step\n",
      "Epoch 16/1000\n",
      "4208/4208 - 74s - loss: 13.7747 - val_loss: 3.3863 - 74s/epoch - 18ms/step\n",
      "Epoch 17/1000\n",
      "4208/4208 - 73s - loss: 13.5068 - val_loss: 3.3327 - 73s/epoch - 17ms/step\n",
      "Epoch 18/1000\n",
      "4208/4208 - 72s - loss: 13.2396 - val_loss: 3.1937 - 72s/epoch - 17ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpwtkj6lc6/model/data/model/assets\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/29 22:08:01 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/29 22:08:01 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4208/4208 - 67s - loss: 23.0762 - val_loss: 82.8756 - 67s/epoch - 16ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 64s - loss: 15.8771 - val_loss: 79.5110 - 64s/epoch - 15ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 64s - loss: 15.3577 - val_loss: 76.5975 - 64s/epoch - 15ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 65s - loss: 15.0387 - val_loss: 71.7499 - 65s/epoch - 15ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 65s - loss: 14.7555 - val_loss: 67.6575 - 65s/epoch - 16ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 65s - loss: 14.4949 - val_loss: 64.1202 - 65s/epoch - 15ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 65s - loss: 14.2486 - val_loss: 54.2958 - 65s/epoch - 15ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 66s - loss: 13.9816 - val_loss: 44.7883 - 66s/epoch - 16ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 65s - loss: 13.6498 - val_loss: 34.6009 - 65s/epoch - 15ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 65s - loss: 13.1513 - val_loss: 23.8670 - 65s/epoch - 15ms/step\n",
      "Epoch 11/1000\n",
      "4208/4208 - 65s - loss: 12.6099 - val_loss: 17.1588 - 65s/epoch - 15ms/step\n",
      "Epoch 12/1000\n",
      "4208/4208 - 64s - loss: 12.1275 - val_loss: 11.9084 - 64s/epoch - 15ms/step\n",
      "Epoch 13/1000\n",
      "4208/4208 - 63s - loss: 11.6604 - val_loss: 7.8672 - 63s/epoch - 15ms/step\n",
      "Epoch 14/1000\n",
      "4208/4208 - 63s - loss: 11.2132 - val_loss: 6.1246 - 63s/epoch - 15ms/step\n",
      "Epoch 15/1000\n",
      "4208/4208 - 63s - loss: 10.8245 - val_loss: 4.8888 - 63s/epoch - 15ms/step\n",
      "Epoch 16/1000\n",
      "4208/4208 - 64s - loss: 10.4560 - val_loss: 3.5988 - 64s/epoch - 15ms/step\n",
      "Epoch 17/1000\n",
      "4208/4208 - 63s - loss: 9.9728 - val_loss: 2.9813 - 63s/epoch - 15ms/step\n",
      "Epoch 18/1000\n",
      "4208/4208 - 64s - loss: 9.6990 - val_loss: 2.9903 - 64s/epoch - 15ms/step\n",
      "Epoch 19/1000\n",
      "4208/4208 - 65s - loss: 9.4674 - val_loss: 2.9618 - 65s/epoch - 15ms/step\n",
      "Epoch 20/1000\n",
      "4208/4208 - 65s - loss: 9.2543 - val_loss: 2.8898 - 65s/epoch - 16ms/step\n",
      "Epoch 21/1000\n",
      "4208/4208 - 63s - loss: 9.0595 - val_loss: 2.9142 - 63s/epoch - 15ms/step\n",
      "Epoch 22/1000\n",
      "4208/4208 - 65s - loss: 8.8723 - val_loss: 2.7085 - 65s/epoch - 15ms/step\n",
      "Epoch 23/1000\n",
      "4208/4208 - 65s - loss: 8.6979 - val_loss: 2.7628 - 65s/epoch - 15ms/step\n",
      "Epoch 24/1000\n",
      "4208/4208 - 64s - loss: 8.5317 - val_loss: 2.6617 - 64s/epoch - 15ms/step\n",
      "Epoch 25/1000\n",
      "4208/4208 - 62s - loss: 8.3710 - val_loss: 2.6972 - 62s/epoch - 15ms/step\n",
      "Epoch 26/1000\n",
      "4208/4208 - 63s - loss: 8.2093 - val_loss: 2.5830 - 63s/epoch - 15ms/step\n",
      "Epoch 27/1000\n",
      "4208/4208 - 65s - loss: 8.0518 - val_loss: 2.5001 - 65s/epoch - 15ms/step\n",
      "Epoch 28/1000\n",
      "4208/4208 - 64s - loss: 7.8885 - val_loss: 2.6846 - 64s/epoch - 15ms/step\n",
      "Epoch 29/1000\n",
      "4208/4208 - 64s - loss: 7.7286 - val_loss: 2.6284 - 64s/epoch - 15ms/step\n",
      "Epoch 30/1000\n",
      "4208/4208 - 65s - loss: 7.5713 - val_loss: 2.4770 - 65s/epoch - 15ms/step\n",
      "Epoch 31/1000\n",
      "4208/4208 - 65s - loss: 7.4118 - val_loss: 2.4329 - 65s/epoch - 15ms/step\n",
      "Epoch 32/1000\n",
      "4208/4208 - 65s - loss: 7.2514 - val_loss: 2.5338 - 65s/epoch - 15ms/step\n",
      "Epoch 33/1000\n",
      "4208/4208 - 64s - loss: 7.0969 - val_loss: 2.5223 - 64s/epoch - 15ms/step\n",
      "Epoch 34/1000\n",
      "4208/4208 - 65s - loss: 6.9396 - val_loss: 2.4743 - 65s/epoch - 15ms/step\n",
      "Epoch 35/1000\n",
      "4208/4208 - 65s - loss: 6.7816 - val_loss: 2.3653 - 65s/epoch - 15ms/step\n",
      "Epoch 36/1000\n",
      "4208/4208 - 65s - loss: 6.6243 - val_loss: 2.3649 - 65s/epoch - 15ms/step\n",
      "Epoch 37/1000\n",
      "4208/4208 - 65s - loss: 6.4724 - val_loss: 2.4656 - 65s/epoch - 15ms/step\n",
      "Epoch 38/1000\n",
      "4208/4208 - 65s - loss: 6.3144 - val_loss: 2.3592 - 65s/epoch - 15ms/step\n",
      "Epoch 39/1000\n",
      "4208/4208 - 64s - loss: 6.1622 - val_loss: 2.3472 - 64s/epoch - 15ms/step\n",
      "Epoch 40/1000\n",
      "4208/4208 - 64s - loss: 6.0084 - val_loss: 2.3917 - 64s/epoch - 15ms/step\n",
      "Epoch 41/1000\n",
      "4208/4208 - 65s - loss: 5.8570 - val_loss: 2.3582 - 65s/epoch - 15ms/step\n",
      "Epoch 42/1000\n",
      "4208/4208 - 66s - loss: 5.7067 - val_loss: 2.4079 - 66s/epoch - 16ms/step\n",
      "Epoch 43/1000\n",
      "4208/4208 - 65s - loss: 5.5537 - val_loss: 2.2948 - 65s/epoch - 15ms/step\n",
      "Epoch 44/1000\n",
      "4208/4208 - 65s - loss: 5.4042 - val_loss: 2.4087 - 65s/epoch - 15ms/step\n",
      "Epoch 45/1000\n",
      "4208/4208 - 64s - loss: 5.2549 - val_loss: 2.2275 - 64s/epoch - 15ms/step\n",
      "Epoch 46/1000\n",
      "4208/4208 - 63s - loss: 5.1046 - val_loss: 2.2695 - 63s/epoch - 15ms/step\n",
      "Epoch 47/1000\n",
      "4208/4208 - 62s - loss: 4.9611 - val_loss: 2.2567 - 62s/epoch - 15ms/step\n",
      "Epoch 48/1000\n",
      "4208/4208 - 62s - loss: 4.8160 - val_loss: 2.3297 - 62s/epoch - 15ms/step\n",
      "Epoch 49/1000\n",
      "4208/4208 - 62s - loss: 4.6744 - val_loss: 2.1731 - 62s/epoch - 15ms/step\n",
      "Epoch 50/1000\n",
      "4208/4208 - 62s - loss: 4.5335 - val_loss: 2.3576 - 62s/epoch - 15ms/step\n",
      "Epoch 51/1000\n",
      "4208/4208 - 61s - loss: 4.3939 - val_loss: 2.2046 - 61s/epoch - 14ms/step\n",
      "Epoch 52/1000\n",
      "4208/4208 - 63s - loss: 4.2585 - val_loss: 2.3478 - 63s/epoch - 15ms/step\n",
      "Epoch 53/1000\n",
      "4208/4208 - 63s - loss: 4.1238 - val_loss: 2.2125 - 63s/epoch - 15ms/step\n",
      "Epoch 54/1000\n",
      "4208/4208 - 63s - loss: 3.9950 - val_loss: 2.1961 - 63s/epoch - 15ms/step\n",
      "Epoch 55/1000\n",
      "4208/4208 - 63s - loss: 3.8664 - val_loss: 2.1866 - 63s/epoch - 15ms/step\n",
      "Epoch 56/1000\n",
      "4208/4208 - 64s - loss: 3.7374 - val_loss: 2.1113 - 64s/epoch - 15ms/step\n",
      "Epoch 57/1000\n",
      "4208/4208 - 64s - loss: 3.6123 - val_loss: 2.0877 - 64s/epoch - 15ms/step\n",
      "Epoch 58/1000\n",
      "4208/4208 - 64s - loss: 3.4946 - val_loss: 2.0642 - 64s/epoch - 15ms/step\n",
      "Epoch 59/1000\n",
      "4208/4208 - 64s - loss: 3.3800 - val_loss: 2.0757 - 64s/epoch - 15ms/step\n",
      "Epoch 60/1000\n",
      "4208/4208 - 64s - loss: 3.2677 - val_loss: 2.0910 - 64s/epoch - 15ms/step\n",
      "Epoch 61/1000\n",
      "4208/4208 - 65s - loss: 3.1628 - val_loss: 2.0731 - 65s/epoch - 15ms/step\n",
      "Epoch 62/1000\n",
      "4208/4208 - 63s - loss: 3.0611 - val_loss: 2.0295 - 63s/epoch - 15ms/step\n",
      "Epoch 63/1000\n",
      "4208/4208 - 63s - loss: 2.9631 - val_loss: 2.0843 - 63s/epoch - 15ms/step\n",
      "Epoch 64/1000\n",
      "4208/4208 - 63s - loss: 2.8674 - val_loss: 2.0801 - 63s/epoch - 15ms/step\n",
      "Epoch 65/1000\n",
      "4208/4208 - 63s - loss: 2.7751 - val_loss: 2.0080 - 63s/epoch - 15ms/step\n",
      "Epoch 66/1000\n",
      "4208/4208 - 65s - loss: 2.6872 - val_loss: 2.0161 - 65s/epoch - 15ms/step\n",
      "Epoch 67/1000\n",
      "4208/4208 - 65s - loss: 2.6027 - val_loss: 1.9756 - 65s/epoch - 15ms/step\n",
      "Epoch 68/1000\n",
      "4208/4208 - 64s - loss: 2.5229 - val_loss: 1.9655 - 64s/epoch - 15ms/step\n",
      "Epoch 69/1000\n",
      "4208/4208 - 64s - loss: 2.4462 - val_loss: 1.9572 - 64s/epoch - 15ms/step\n",
      "Epoch 70/1000\n",
      "4208/4208 - 66s - loss: 2.3729 - val_loss: 1.9181 - 66s/epoch - 16ms/step\n",
      "Epoch 71/1000\n",
      "4208/4208 - 64s - loss: 2.3005 - val_loss: 1.9409 - 64s/epoch - 15ms/step\n",
      "Epoch 72/1000\n",
      "4208/4208 - 63s - loss: 2.2316 - val_loss: 1.8813 - 63s/epoch - 15ms/step\n",
      "Epoch 73/1000\n",
      "4208/4208 - 64s - loss: 2.1637 - val_loss: 1.8719 - 64s/epoch - 15ms/step\n",
      "Epoch 74/1000\n",
      "4208/4208 - 65s - loss: 2.1015 - val_loss: 1.8550 - 65s/epoch - 15ms/step\n",
      "Epoch 75/1000\n",
      "4208/4208 - 64s - loss: 2.0594 - val_loss: 1.8487 - 64s/epoch - 15ms/step\n",
      "Epoch 76/1000\n",
      "4208/4208 - 62s - loss: 1.9908 - val_loss: 1.8220 - 62s/epoch - 15ms/step\n",
      "Epoch 77/1000\n",
      "4208/4208 - 64s - loss: 1.9383 - val_loss: 1.7895 - 64s/epoch - 15ms/step\n",
      "Epoch 78/1000\n",
      "4208/4208 - 63s - loss: 1.8874 - val_loss: 1.7991 - 63s/epoch - 15ms/step\n",
      "Epoch 79/1000\n",
      "4208/4208 - 63s - loss: 1.8420 - val_loss: 1.8131 - 63s/epoch - 15ms/step\n",
      "Epoch 80/1000\n",
      "4208/4208 - 62s - loss: 1.8023 - val_loss: 1.8385 - 62s/epoch - 15ms/step\n",
      "Epoch 81/1000\n",
      "4208/4208 - 61s - loss: 1.7678 - val_loss: 1.8250 - 61s/epoch - 15ms/step\n",
      "Epoch 82/1000\n",
      "4208/4208 - 64s - loss: 1.7433 - val_loss: 1.8403 - 64s/epoch - 15ms/step\n",
      "Epoch 83/1000\n",
      "4208/4208 - 63s - loss: 1.7143 - val_loss: 1.8135 - 63s/epoch - 15ms/step\n",
      "Epoch 84/1000\n",
      "4208/4208 - 64s - loss: 1.6895 - val_loss: 1.8351 - 64s/epoch - 15ms/step\n",
      "Epoch 85/1000\n",
      "4208/4208 - 64s - loss: 1.7137 - val_loss: 1.8780 - 64s/epoch - 15ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmppfclbk9o/model/data/model/assets\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/29 23:44:37 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/29 23:44:37 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0116s vs `on_train_batch_end` time: 0.1345s). Check your callbacks.\n",
      "4208/4208 - 62s - loss: 54.1343 - val_loss: 27.5678 - 62s/epoch - 15ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 60s - loss: 35.2138 - val_loss: 8.6915 - 60s/epoch - 14ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 61s - loss: 33.6544 - val_loss: 6.5966 - 61s/epoch - 14ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 61s - loss: 33.0350 - val_loss: 8.3694 - 61s/epoch - 14ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 61s - loss: 32.4472 - val_loss: 7.1464 - 61s/epoch - 14ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 61s - loss: 31.8794 - val_loss: 8.0719 - 61s/epoch - 14ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 60s - loss: 31.2983 - val_loss: 7.5150 - 60s/epoch - 14ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 58s - loss: 30.7157 - val_loss: 7.2136 - 58s/epoch - 14ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 59s - loss: 30.1479 - val_loss: 6.9073 - 59s/epoch - 14ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 60s - loss: 29.5793 - val_loss: 6.8589 - 60s/epoch - 14ms/step\n",
      "Epoch 11/1000\n",
      "4208/4208 - 59s - loss: 28.9892 - val_loss: 6.9688 - 59s/epoch - 14ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmplxu7ce2p/model/data/model/assets\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/30 00:01:25 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/30 00:01:25 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4208/4208 - 61s - loss: 13.4683 - val_loss: 38.8202 - 61s/epoch - 14ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 61s - loss: 8.3675 - val_loss: 29.6031 - 61s/epoch - 15ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 60s - loss: 7.2843 - val_loss: 17.1520 - 60s/epoch - 14ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 58s - loss: 6.9167 - val_loss: 13.5880 - 58s/epoch - 14ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 59s - loss: 6.7443 - val_loss: 11.2717 - 59s/epoch - 14ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 59s - loss: 6.6368 - val_loss: 9.9399 - 59s/epoch - 14ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 60s - loss: 6.5525 - val_loss: 7.5683 - 60s/epoch - 14ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 59s - loss: 6.4103 - val_loss: 7.6240 - 59s/epoch - 14ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 59s - loss: 6.3271 - val_loss: 5.3699 - 59s/epoch - 14ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 58s - loss: 6.2530 - val_loss: 4.7295 - 58s/epoch - 14ms/step\n",
      "Epoch 11/1000\n",
      "4208/4208 - 60s - loss: 6.1287 - val_loss: 2.7637 - 60s/epoch - 14ms/step\n",
      "Epoch 12/1000\n",
      "4208/4208 - 60s - loss: 6.0685 - val_loss: 2.4241 - 60s/epoch - 14ms/step\n",
      "Epoch 13/1000\n",
      "4208/4208 - 59s - loss: 5.9934 - val_loss: 2.1346 - 59s/epoch - 14ms/step\n",
      "Epoch 14/1000\n",
      "4208/4208 - 58s - loss: 5.9217 - val_loss: 1.8492 - 58s/epoch - 14ms/step\n",
      "Epoch 15/1000\n",
      "4208/4208 - 59s - loss: 5.8679 - val_loss: 1.8988 - 59s/epoch - 14ms/step\n",
      "Epoch 16/1000\n",
      "4208/4208 - 60s - loss: 5.8080 - val_loss: 1.9744 - 60s/epoch - 14ms/step\n",
      "Epoch 17/1000\n",
      "4208/4208 - 58s - loss: 5.7267 - val_loss: 2.0932 - 58s/epoch - 14ms/step\n",
      "Epoch 18/1000\n",
      "4208/4208 - 58s - loss: 5.6695 - val_loss: 1.7907 - 58s/epoch - 14ms/step\n",
      "Epoch 19/1000\n",
      "4208/4208 - 59s - loss: 5.6187 - val_loss: 1.8591 - 59s/epoch - 14ms/step\n",
      "Epoch 20/1000\n",
      "4208/4208 - 59s - loss: 5.5691 - val_loss: 1.7963 - 59s/epoch - 14ms/step\n",
      "Epoch 21/1000\n",
      "4208/4208 - 58s - loss: 5.5145 - val_loss: 1.8054 - 58s/epoch - 14ms/step\n",
      "Epoch 22/1000\n",
      "4208/4208 - 58s - loss: 5.4500 - val_loss: 1.7987 - 58s/epoch - 14ms/step\n",
      "Epoch 23/1000\n",
      "4208/4208 - 58s - loss: 5.3954 - val_loss: 1.7809 - 58s/epoch - 14ms/step\n",
      "Epoch 24/1000\n",
      "4208/4208 - 57s - loss: 5.3472 - val_loss: 1.8966 - 57s/epoch - 13ms/step\n",
      "Epoch 25/1000\n",
      "4208/4208 - 58s - loss: 5.2948 - val_loss: 1.7350 - 58s/epoch - 14ms/step\n",
      "Epoch 26/1000\n",
      "4208/4208 - 56s - loss: 5.2455 - val_loss: 1.7448 - 56s/epoch - 13ms/step\n",
      "Epoch 27/1000\n",
      "4208/4208 - 55s - loss: 5.1942 - val_loss: 1.8565 - 55s/epoch - 13ms/step\n",
      "Epoch 28/1000\n",
      "4208/4208 - 57s - loss: 5.1452 - val_loss: 1.8134 - 57s/epoch - 13ms/step\n",
      "Epoch 29/1000\n",
      "4208/4208 - 59s - loss: 5.0912 - val_loss: 1.7910 - 59s/epoch - 14ms/step\n",
      "Epoch 30/1000\n",
      "4208/4208 - 58s - loss: 5.0434 - val_loss: 1.7404 - 58s/epoch - 14ms/step\n",
      "Epoch 31/1000\n",
      "4208/4208 - 57s - loss: 4.9934 - val_loss: 1.6923 - 57s/epoch - 14ms/step\n",
      "Epoch 32/1000\n",
      "4208/4208 - 57s - loss: 4.9418 - val_loss: 1.7213 - 57s/epoch - 14ms/step\n",
      "Epoch 33/1000\n",
      "4208/4208 - 57s - loss: 4.8912 - val_loss: 1.9593 - 57s/epoch - 14ms/step\n",
      "Epoch 34/1000\n",
      "4208/4208 - 57s - loss: 4.8424 - val_loss: 1.7379 - 57s/epoch - 14ms/step\n",
      "Epoch 35/1000\n",
      "4208/4208 - 58s - loss: 4.7917 - val_loss: 1.7755 - 58s/epoch - 14ms/step\n",
      "Epoch 36/1000\n",
      "4208/4208 - 58s - loss: 4.7407 - val_loss: 1.6734 - 58s/epoch - 14ms/step\n",
      "Epoch 37/1000\n",
      "4208/4208 - 58s - loss: 4.6897 - val_loss: 1.7786 - 58s/epoch - 14ms/step\n",
      "Epoch 38/1000\n",
      "4208/4208 - 60s - loss: 4.6395 - val_loss: 1.7780 - 60s/epoch - 14ms/step\n",
      "Epoch 39/1000\n",
      "4208/4208 - 58s - loss: 4.5901 - val_loss: 1.7398 - 58s/epoch - 14ms/step\n",
      "Epoch 40/1000\n",
      "4208/4208 - 58s - loss: 4.5392 - val_loss: 1.6899 - 58s/epoch - 14ms/step\n",
      "Epoch 41/1000\n",
      "4208/4208 - 58s - loss: 4.4908 - val_loss: 1.8060 - 58s/epoch - 14ms/step\n",
      "Epoch 42/1000\n",
      "4208/4208 - 58s - loss: 4.4412 - val_loss: 1.7092 - 58s/epoch - 14ms/step\n",
      "Epoch 43/1000\n",
      "4208/4208 - 59s - loss: 4.3904 - val_loss: 1.6965 - 59s/epoch - 14ms/step\n",
      "Epoch 44/1000\n",
      "4208/4208 - 58s - loss: 4.3413 - val_loss: 1.7474 - 58s/epoch - 14ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ef8387b8d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpom2btix3/model/data/model/assets\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/30 00:49:33 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/30 00:49:33 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4208/4208 - 65s - loss: 9.8670 - val_loss: 6.9518 - 65s/epoch - 15ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 63s - loss: 4.7081 - val_loss: 7.3880 - 63s/epoch - 15ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 63s - loss: 4.4406 - val_loss: 5.6030 - 63s/epoch - 15ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 64s - loss: 4.3621 - val_loss: 6.3253 - 64s/epoch - 15ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 63s - loss: 4.3027 - val_loss: 6.5602 - 63s/epoch - 15ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 63s - loss: 4.2229 - val_loss: 5.8129 - 63s/epoch - 15ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 65s - loss: 4.0828 - val_loss: 6.4632 - 65s/epoch - 15ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 65s - loss: 3.9916 - val_loss: 6.6799 - 65s/epoch - 15ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 64s - loss: 3.9281 - val_loss: 6.6180 - 64s/epoch - 15ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 62s - loss: 3.8866 - val_loss: 5.4781 - 62s/epoch - 15ms/step\n",
      "Epoch 11/1000\n",
      "4208/4208 - 63s - loss: 3.8464 - val_loss: 5.9707 - 63s/epoch - 15ms/step\n",
      "Epoch 12/1000\n",
      "4208/4208 - 62s - loss: 3.8110 - val_loss: 7.1546 - 62s/epoch - 15ms/step\n",
      "Epoch 13/1000\n",
      "4208/4208 - 62s - loss: 3.7595 - val_loss: 5.2557 - 62s/epoch - 15ms/step\n",
      "Epoch 14/1000\n",
      "4208/4208 - 63s - loss: 3.7254 - val_loss: 6.9605 - 63s/epoch - 15ms/step\n",
      "Epoch 15/1000\n",
      "4208/4208 - 63s - loss: 3.6928 - val_loss: 6.0641 - 63s/epoch - 15ms/step\n",
      "Epoch 16/1000\n",
      "4208/4208 - 63s - loss: 3.6592 - val_loss: 5.2568 - 63s/epoch - 15ms/step\n",
      "Epoch 17/1000\n",
      "4208/4208 - 63s - loss: 3.6317 - val_loss: 5.8065 - 63s/epoch - 15ms/step\n",
      "Epoch 18/1000\n",
      "4208/4208 - 62s - loss: 3.6083 - val_loss: 5.0416 - 62s/epoch - 15ms/step\n",
      "Epoch 19/1000\n",
      "4208/4208 - 63s - loss: 3.5874 - val_loss: 6.0278 - 63s/epoch - 15ms/step\n",
      "Epoch 20/1000\n",
      "4208/4208 - 63s - loss: 3.5705 - val_loss: 6.7820 - 63s/epoch - 15ms/step\n",
      "Epoch 21/1000\n",
      "4208/4208 - 64s - loss: 3.5570 - val_loss: 5.9810 - 64s/epoch - 15ms/step\n",
      "Epoch 22/1000\n",
      "4208/4208 - 61s - loss: 3.5388 - val_loss: 5.7844 - 61s/epoch - 15ms/step\n",
      "Epoch 23/1000\n",
      "4208/4208 - 61s - loss: 3.5211 - val_loss: 6.0922 - 61s/epoch - 15ms/step\n",
      "Epoch 24/1000\n",
      "4208/4208 - 62s - loss: 3.5041 - val_loss: 5.9335 - 62s/epoch - 15ms/step\n",
      "Epoch 25/1000\n",
      "4208/4208 - 62s - loss: 3.4906 - val_loss: 6.9084 - 62s/epoch - 15ms/step\n",
      "Epoch 26/1000\n",
      "4208/4208 - 61s - loss: 3.4751 - val_loss: 5.9947 - 61s/epoch - 15ms/step\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ef83aef8700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp6_eixiaq/model/data/model/assets\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/30 01:22:31 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/30 01:22:31 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4208/4208 - 57s - loss: 28.5765 - val_loss: 59.5559 - 57s/epoch - 14ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 56s - loss: 21.2763 - val_loss: 48.8379 - 56s/epoch - 13ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 56s - loss: 18.6018 - val_loss: 13.7812 - 56s/epoch - 13ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 56s - loss: 17.1833 - val_loss: 3.9100 - 56s/epoch - 13ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 57s - loss: 16.7410 - val_loss: 4.3064 - 57s/epoch - 14ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 56s - loss: 16.4505 - val_loss: 3.7303 - 56s/epoch - 13ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 56s - loss: 16.1778 - val_loss: 3.7950 - 56s/epoch - 13ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 56s - loss: 15.8980 - val_loss: 3.4777 - 56s/epoch - 13ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 58s - loss: 15.6289 - val_loss: 3.4730 - 58s/epoch - 14ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 57s - loss: 15.3546 - val_loss: 3.2755 - 57s/epoch - 14ms/step\n",
      "Epoch 11/1000\n",
      "4208/4208 - 57s - loss: 15.0864 - val_loss: 3.5106 - 57s/epoch - 14ms/step\n",
      "Epoch 12/1000\n",
      "4208/4208 - 58s - loss: 14.8135 - val_loss: 3.4848 - 58s/epoch - 14ms/step\n",
      "Epoch 13/1000\n",
      "4208/4208 - 58s - loss: 14.5475 - val_loss: 3.5386 - 58s/epoch - 14ms/step\n",
      "Epoch 14/1000\n",
      "4208/4208 - 57s - loss: 14.2812 - val_loss: 3.5158 - 57s/epoch - 13ms/step\n",
      "Epoch 15/1000\n",
      "4208/4208 - 56s - loss: 14.0106 - val_loss: 3.2009 - 56s/epoch - 13ms/step\n",
      "Epoch 16/1000\n",
      "4208/4208 - 55s - loss: 13.7349 - val_loss: 3.4516 - 55s/epoch - 13ms/step\n",
      "Epoch 17/1000\n",
      "4208/4208 - 56s - loss: 13.4739 - val_loss: 3.2697 - 56s/epoch - 13ms/step\n",
      "Epoch 18/1000\n",
      "4208/4208 - 55s - loss: 13.1990 - val_loss: 3.3698 - 55s/epoch - 13ms/step\n",
      "Epoch 19/1000\n",
      "4208/4208 - 56s - loss: 12.9338 - val_loss: 3.3518 - 56s/epoch - 13ms/step\n",
      "Epoch 20/1000\n",
      "4208/4208 - 57s - loss: 12.6641 - val_loss: 3.2166 - 57s/epoch - 14ms/step\n",
      "Epoch 21/1000\n",
      "4208/4208 - 54s - loss: 12.3984 - val_loss: 3.3661 - 54s/epoch - 13ms/step\n",
      "Epoch 22/1000\n",
      "4208/4208 - 55s - loss: 12.1241 - val_loss: 3.1999 - 55s/epoch - 13ms/step\n",
      "Epoch 23/1000\n",
      "4208/4208 - 55s - loss: 11.8609 - val_loss: 3.2070 - 55s/epoch - 13ms/step\n",
      "Epoch 24/1000\n",
      "4208/4208 - 56s - loss: 11.5913 - val_loss: 3.0929 - 56s/epoch - 13ms/step\n",
      "Epoch 25/1000\n",
      "4208/4208 - 56s - loss: 11.3267 - val_loss: 3.1293 - 56s/epoch - 13ms/step\n",
      "Epoch 26/1000\n",
      "4208/4208 - 56s - loss: 11.0579 - val_loss: 3.1233 - 56s/epoch - 13ms/step\n",
      "Epoch 27/1000\n",
      "4208/4208 - 55s - loss: 10.7937 - val_loss: 3.0186 - 55s/epoch - 13ms/step\n",
      "Epoch 28/1000\n",
      "4208/4208 - 56s - loss: 10.5258 - val_loss: 3.1117 - 56s/epoch - 13ms/step\n",
      "Epoch 29/1000\n",
      "4208/4208 - 58s - loss: 10.2641 - val_loss: 3.1785 - 58s/epoch - 14ms/step\n",
      "Epoch 30/1000\n",
      "4208/4208 - 56s - loss: 9.9966 - val_loss: 3.1109 - 56s/epoch - 13ms/step\n",
      "Epoch 31/1000\n",
      "4208/4208 - 55s - loss: 9.7369 - val_loss: 3.0238 - 55s/epoch - 13ms/step\n",
      "Epoch 32/1000\n",
      "4208/4208 - 56s - loss: 9.4737 - val_loss: 3.0021 - 56s/epoch - 13ms/step\n",
      "Epoch 33/1000\n",
      "4208/4208 - 55s - loss: 9.2130 - val_loss: 2.9634 - 55s/epoch - 13ms/step\n",
      "Epoch 34/1000\n",
      "4208/4208 - 55s - loss: 8.9514 - val_loss: 2.8952 - 55s/epoch - 13ms/step\n",
      "Epoch 35/1000\n",
      "4208/4208 - 57s - loss: 8.6905 - val_loss: 2.9020 - 57s/epoch - 13ms/step\n",
      "Epoch 36/1000\n",
      "4208/4208 - 56s - loss: 8.4272 - val_loss: 2.9590 - 56s/epoch - 13ms/step\n",
      "Epoch 37/1000\n",
      "4208/4208 - 55s - loss: 8.1671 - val_loss: 2.8633 - 55s/epoch - 13ms/step\n",
      "Epoch 38/1000\n",
      "4208/4208 - 56s - loss: 7.9125 - val_loss: 2.8972 - 56s/epoch - 13ms/step\n",
      "Epoch 39/1000\n",
      "4208/4208 - 56s - loss: 7.6517 - val_loss: 2.7695 - 56s/epoch - 13ms/step\n",
      "Epoch 40/1000\n",
      "4208/4208 - 56s - loss: 7.4021 - val_loss: 2.9304 - 56s/epoch - 13ms/step\n",
      "Epoch 41/1000\n",
      "4208/4208 - 54s - loss: 7.1485 - val_loss: 2.7054 - 54s/epoch - 13ms/step\n",
      "Epoch 42/1000\n",
      "4208/4208 - 54s - loss: 6.8968 - val_loss: 2.6835 - 54s/epoch - 13ms/step\n",
      "Epoch 43/1000\n",
      "4208/4208 - 55s - loss: 6.6450 - val_loss: 2.7974 - 55s/epoch - 13ms/step\n",
      "Epoch 44/1000\n",
      "4208/4208 - 57s - loss: 6.3966 - val_loss: 2.6890 - 57s/epoch - 13ms/step\n",
      "Epoch 45/1000\n",
      "4208/4208 - 56s - loss: 6.1481 - val_loss: 2.5021 - 56s/epoch - 13ms/step\n",
      "Epoch 46/1000\n",
      "4208/4208 - 56s - loss: 5.9072 - val_loss: 2.5253 - 56s/epoch - 13ms/step\n",
      "Epoch 47/1000\n",
      "4208/4208 - 55s - loss: 5.6656 - val_loss: 2.4970 - 55s/epoch - 13ms/step\n",
      "Epoch 48/1000\n",
      "4208/4208 - 56s - loss: 5.4311 - val_loss: 2.5041 - 56s/epoch - 13ms/step\n",
      "Epoch 49/1000\n",
      "4208/4208 - 56s - loss: 5.2028 - val_loss: 2.5181 - 56s/epoch - 13ms/step\n",
      "Epoch 50/1000\n",
      "4208/4208 - 56s - loss: 4.9786 - val_loss: 2.4645 - 56s/epoch - 13ms/step\n",
      "Epoch 51/1000\n",
      "4208/4208 - 57s - loss: 4.7613 - val_loss: 2.4402 - 57s/epoch - 13ms/step\n",
      "Epoch 52/1000\n",
      "4208/4208 - 56s - loss: 4.5525 - val_loss: 2.4785 - 56s/epoch - 13ms/step\n",
      "Epoch 53/1000\n",
      "4208/4208 - 56s - loss: 4.3511 - val_loss: 2.3389 - 56s/epoch - 13ms/step\n",
      "Epoch 54/1000\n",
      "4208/4208 - 55s - loss: 4.1607 - val_loss: 2.3772 - 55s/epoch - 13ms/step\n",
      "Epoch 55/1000\n",
      "4208/4208 - 56s - loss: 3.9776 - val_loss: 2.3360 - 56s/epoch - 13ms/step\n",
      "Epoch 56/1000\n",
      "4208/4208 - 56s - loss: 3.8041 - val_loss: 2.2711 - 56s/epoch - 13ms/step\n",
      "Epoch 57/1000\n",
      "4208/4208 - 55s - loss: 3.6389 - val_loss: 2.3033 - 55s/epoch - 13ms/step\n",
      "Epoch 58/1000\n",
      "4208/4208 - 57s - loss: 3.4856 - val_loss: 2.2872 - 57s/epoch - 13ms/step\n",
      "Epoch 59/1000\n",
      "4208/4208 - 58s - loss: 3.3384 - val_loss: 2.2127 - 58s/epoch - 14ms/step\n",
      "Epoch 60/1000\n",
      "4208/4208 - 56s - loss: 3.1970 - val_loss: 2.2183 - 56s/epoch - 13ms/step\n",
      "Epoch 61/1000\n",
      "4208/4208 - 57s - loss: 3.0614 - val_loss: 2.1686 - 57s/epoch - 13ms/step\n",
      "Epoch 62/1000\n",
      "4208/4208 - 56s - loss: 2.9316 - val_loss: 2.1122 - 56s/epoch - 13ms/step\n",
      "Epoch 63/1000\n",
      "4208/4208 - 57s - loss: 2.8077 - val_loss: 2.1194 - 57s/epoch - 14ms/step\n",
      "Epoch 64/1000\n",
      "4208/4208 - 56s - loss: 2.6927 - val_loss: 2.1108 - 56s/epoch - 13ms/step\n",
      "Epoch 65/1000\n",
      "4208/4208 - 57s - loss: 2.5847 - val_loss: 2.0618 - 57s/epoch - 13ms/step\n",
      "Epoch 66/1000\n",
      "4208/4208 - 56s - loss: 2.4836 - val_loss: 2.0382 - 56s/epoch - 13ms/step\n",
      "Epoch 67/1000\n",
      "4208/4208 - 56s - loss: 2.3912 - val_loss: 2.0342 - 56s/epoch - 13ms/step\n",
      "Epoch 68/1000\n",
      "4208/4208 - 56s - loss: 2.3063 - val_loss: 2.0025 - 56s/epoch - 13ms/step\n",
      "Epoch 69/1000\n",
      "4208/4208 - 57s - loss: 2.2283 - val_loss: 1.9716 - 57s/epoch - 14ms/step\n",
      "Epoch 70/1000\n",
      "4208/4208 - 56s - loss: 2.1579 - val_loss: 1.9621 - 56s/epoch - 13ms/step\n",
      "Epoch 71/1000\n",
      "4208/4208 - 57s - loss: 2.0985 - val_loss: 1.9637 - 57s/epoch - 14ms/step\n",
      "Epoch 72/1000\n",
      "4208/4208 - 57s - loss: 2.0490 - val_loss: 1.9600 - 57s/epoch - 13ms/step\n",
      "Epoch 73/1000\n",
      "4208/4208 - 57s - loss: 2.0102 - val_loss: 1.9552 - 57s/epoch - 14ms/step\n",
      "Epoch 74/1000\n",
      "4208/4208 - 57s - loss: 1.9811 - val_loss: 1.9487 - 57s/epoch - 14ms/step\n",
      "Epoch 75/1000\n",
      "4208/4208 - 57s - loss: 1.9620 - val_loss: 1.9576 - 57s/epoch - 14ms/step\n",
      "Epoch 76/1000\n",
      "4208/4208 - 58s - loss: 1.9494 - val_loss: 1.9879 - 58s/epoch - 14ms/step\n",
      "Epoch 77/1000\n",
      "4208/4208 - 56s - loss: 1.9398 - val_loss: 1.9908 - 56s/epoch - 13ms/step\n",
      "Epoch 78/1000\n",
      "4208/4208 - 57s - loss: 1.9353 - val_loss: 1.9848 - 57s/epoch - 14ms/step\n",
      "Epoch 79/1000\n",
      "4208/4208 - 56s - loss: 1.9310 - val_loss: 2.0129 - 56s/epoch - 13ms/step\n",
      "Epoch 80/1000\n",
      "4208/4208 - 57s - loss: 1.9253 - val_loss: 2.0184 - 57s/epoch - 14ms/step\n",
      "Epoch 81/1000\n",
      "4208/4208 - 57s - loss: 1.9228 - val_loss: 2.0143 - 57s/epoch - 14ms/step\n",
      "Epoch 82/1000\n",
      "4208/4208 - 56s - loss: 1.9202 - val_loss: 2.0230 - 56s/epoch - 13ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp0ex1ueh2/model/data/model/assets\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/30 02:44:34 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/30 02:44:34 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4208/4208 - 63s - loss: 8.5709 - val_loss: 7.6141 - 63s/epoch - 15ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 62s - loss: 4.4869 - val_loss: 7.4058 - 62s/epoch - 15ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 62s - loss: 4.3008 - val_loss: 8.6374 - 62s/epoch - 15ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 63s - loss: 4.1191 - val_loss: 7.3620 - 63s/epoch - 15ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 61s - loss: 4.0172 - val_loss: 6.2159 - 61s/epoch - 15ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 62s - loss: 3.9586 - val_loss: 6.5952 - 62s/epoch - 15ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 62s - loss: 3.8756 - val_loss: 7.1524 - 62s/epoch - 15ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 63s - loss: 3.7990 - val_loss: 7.0412 - 63s/epoch - 15ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 63s - loss: 3.7513 - val_loss: 6.4121 - 63s/epoch - 15ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 63s - loss: 3.7064 - val_loss: 6.5463 - 63s/epoch - 15ms/step\n",
      "Epoch 11/1000\n",
      "4208/4208 - 63s - loss: 3.6659 - val_loss: 7.8139 - 63s/epoch - 15ms/step\n",
      "Epoch 12/1000\n",
      "4208/4208 - 64s - loss: 3.6329 - val_loss: 6.0483 - 64s/epoch - 15ms/step\n",
      "Epoch 13/1000\n",
      "4208/4208 - 64s - loss: 3.5976 - val_loss: 5.4680 - 64s/epoch - 15ms/step\n",
      "Epoch 14/1000\n",
      "4208/4208 - 63s - loss: 3.5629 - val_loss: 6.7836 - 63s/epoch - 15ms/step\n",
      "Epoch 15/1000\n",
      "4208/4208 - 63s - loss: 3.5360 - val_loss: 5.8174 - 63s/epoch - 15ms/step\n",
      "Epoch 16/1000\n",
      "4208/4208 - 63s - loss: 3.5085 - val_loss: 5.9882 - 63s/epoch - 15ms/step\n",
      "Epoch 17/1000\n",
      "4208/4208 - 63s - loss: 3.4797 - val_loss: 6.3643 - 63s/epoch - 15ms/step\n",
      "Epoch 18/1000\n",
      "4208/4208 - 62s - loss: 3.4538 - val_loss: 5.5739 - 62s/epoch - 15ms/step\n",
      "Epoch 19/1000\n",
      "4208/4208 - 63s - loss: 3.4344 - val_loss: 6.0874 - 63s/epoch - 15ms/step\n",
      "Epoch 20/1000\n",
      "4208/4208 - 62s - loss: 3.4151 - val_loss: 6.4295 - 62s/epoch - 15ms/step\n",
      "Epoch 21/1000\n",
      "4208/4208 - 63s - loss: 3.3999 - val_loss: 5.6469 - 63s/epoch - 15ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpi2g6131g/model/data/model/assets\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/30 03:12:17 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/30 03:12:17 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4208/4208 - 59s - loss: 25.4688 - val_loss: 48.4924 - 59s/epoch - 14ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 57s - loss: 18.7489 - val_loss: 48.1145 - 57s/epoch - 14ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 58s - loss: 18.2481 - val_loss: 46.4316 - 58s/epoch - 14ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 57s - loss: 17.8764 - val_loss: 42.6850 - 57s/epoch - 14ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 58s - loss: 17.5425 - val_loss: 41.6380 - 58s/epoch - 14ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 58s - loss: 17.1779 - val_loss: 40.7387 - 58s/epoch - 14ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 58s - loss: 16.7982 - val_loss: 35.5330 - 58s/epoch - 14ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 58s - loss: 16.3466 - val_loss: 29.8163 - 58s/epoch - 14ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 58s - loss: 15.8315 - val_loss: 20.9482 - 58s/epoch - 14ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 58s - loss: 15.2317 - val_loss: 11.7880 - 58s/epoch - 14ms/step\n",
      "Epoch 11/1000\n",
      "4208/4208 - 58s - loss: 14.6816 - val_loss: 6.1161 - 58s/epoch - 14ms/step\n",
      "Epoch 12/1000\n",
      "4208/4208 - 58s - loss: 14.1900 - val_loss: 5.6455 - 58s/epoch - 14ms/step\n",
      "Epoch 13/1000\n",
      "4208/4208 - 58s - loss: 13.8923 - val_loss: 5.8298 - 58s/epoch - 14ms/step\n",
      "Epoch 14/1000\n",
      "4208/4208 - 59s - loss: 13.6300 - val_loss: 5.6778 - 59s/epoch - 14ms/step\n",
      "Epoch 15/1000\n",
      "4208/4208 - 58s - loss: 13.3765 - val_loss: 5.7068 - 58s/epoch - 14ms/step\n",
      "Epoch 16/1000\n",
      "4208/4208 - 58s - loss: 13.1327 - val_loss: 5.4986 - 58s/epoch - 14ms/step\n",
      "Epoch 17/1000\n",
      "4208/4208 - 58s - loss: 12.8767 - val_loss: 5.2233 - 58s/epoch - 14ms/step\n",
      "Epoch 18/1000\n",
      "4208/4208 - 58s - loss: 12.6258 - val_loss: 5.1780 - 58s/epoch - 14ms/step\n",
      "Epoch 19/1000\n",
      "4208/4208 - 58s - loss: 12.3825 - val_loss: 5.0777 - 58s/epoch - 14ms/step\n",
      "Epoch 20/1000\n",
      "4208/4208 - 59s - loss: 12.1287 - val_loss: 5.1160 - 59s/epoch - 14ms/step\n",
      "Epoch 21/1000\n",
      "4208/4208 - 58s - loss: 11.8722 - val_loss: 4.9998 - 58s/epoch - 14ms/step\n",
      "Epoch 22/1000\n",
      "4208/4208 - 59s - loss: 11.6237 - val_loss: 4.8334 - 59s/epoch - 14ms/step\n",
      "Epoch 23/1000\n",
      "4208/4208 - 59s - loss: 11.3764 - val_loss: 4.8438 - 59s/epoch - 14ms/step\n",
      "Epoch 24/1000\n",
      "4208/4208 - 58s - loss: 11.1260 - val_loss: 4.7302 - 58s/epoch - 14ms/step\n",
      "Epoch 25/1000\n",
      "4208/4208 - 57s - loss: 10.8815 - val_loss: 4.7039 - 57s/epoch - 14ms/step\n",
      "Epoch 26/1000\n",
      "4208/4208 - 56s - loss: 10.6318 - val_loss: 4.3608 - 56s/epoch - 13ms/step\n",
      "Epoch 27/1000\n",
      "4208/4208 - 56s - loss: 10.3824 - val_loss: 4.1527 - 56s/epoch - 13ms/step\n",
      "Epoch 28/1000\n",
      "4208/4208 - 56s - loss: 10.1390 - val_loss: 4.2614 - 56s/epoch - 13ms/step\n",
      "Epoch 29/1000\n",
      "4208/4208 - 56s - loss: 9.8884 - val_loss: 3.9776 - 56s/epoch - 13ms/step\n",
      "Epoch 30/1000\n",
      "4208/4208 - 56s - loss: 9.6359 - val_loss: 3.9981 - 56s/epoch - 13ms/step\n",
      "Epoch 31/1000\n",
      "4208/4208 - 56s - loss: 9.3948 - val_loss: 3.9127 - 56s/epoch - 13ms/step\n",
      "Epoch 32/1000\n",
      "4208/4208 - 55s - loss: 9.1473 - val_loss: 3.9295 - 55s/epoch - 13ms/step\n",
      "Epoch 33/1000\n",
      "4208/4208 - 56s - loss: 8.8989 - val_loss: 3.7986 - 56s/epoch - 13ms/step\n",
      "Epoch 34/1000\n",
      "4208/4208 - 57s - loss: 8.6554 - val_loss: 3.8093 - 57s/epoch - 13ms/step\n",
      "Epoch 35/1000\n",
      "4208/4208 - 56s - loss: 8.4126 - val_loss: 3.6158 - 56s/epoch - 13ms/step\n",
      "Epoch 36/1000\n",
      "4208/4208 - 56s - loss: 8.1696 - val_loss: 3.5608 - 56s/epoch - 13ms/step\n",
      "Epoch 37/1000\n",
      "4208/4208 - 56s - loss: 7.9315 - val_loss: 3.4338 - 56s/epoch - 13ms/step\n",
      "Epoch 38/1000\n",
      "4208/4208 - 56s - loss: 7.6898 - val_loss: 3.4235 - 56s/epoch - 13ms/step\n",
      "Epoch 39/1000\n",
      "4208/4208 - 56s - loss: 7.4543 - val_loss: 3.3351 - 56s/epoch - 13ms/step\n",
      "Epoch 40/1000\n",
      "4208/4208 - 56s - loss: 7.2150 - val_loss: 3.2817 - 56s/epoch - 13ms/step\n",
      "Epoch 41/1000\n",
      "4208/4208 - 57s - loss: 6.9783 - val_loss: 3.1636 - 57s/epoch - 13ms/step\n",
      "Epoch 42/1000\n",
      "4208/4208 - 58s - loss: 6.7437 - val_loss: 3.0673 - 58s/epoch - 14ms/step\n",
      "Epoch 43/1000\n",
      "4208/4208 - 59s - loss: 6.5093 - val_loss: 3.0452 - 59s/epoch - 14ms/step\n",
      "Epoch 44/1000\n",
      "4208/4208 - 58s - loss: 6.2795 - val_loss: 2.9699 - 58s/epoch - 14ms/step\n",
      "Epoch 45/1000\n",
      "4208/4208 - 59s - loss: 6.0465 - val_loss: 2.8335 - 59s/epoch - 14ms/step\n",
      "Epoch 46/1000\n",
      "4208/4208 - 58s - loss: 5.8259 - val_loss: 2.8389 - 58s/epoch - 14ms/step\n",
      "Epoch 47/1000\n",
      "4208/4208 - 58s - loss: 5.6009 - val_loss: 2.7270 - 58s/epoch - 14ms/step\n",
      "Epoch 48/1000\n",
      "4208/4208 - 58s - loss: 5.3817 - val_loss: 2.6811 - 58s/epoch - 14ms/step\n",
      "Epoch 49/1000\n",
      "4208/4208 - 59s - loss: 5.1664 - val_loss: 2.5923 - 59s/epoch - 14ms/step\n",
      "Epoch 50/1000\n",
      "4208/4208 - 57s - loss: 4.9550 - val_loss: 2.5320 - 57s/epoch - 14ms/step\n",
      "Epoch 51/1000\n",
      "4208/4208 - 58s - loss: 4.7498 - val_loss: 2.5206 - 58s/epoch - 14ms/step\n",
      "Epoch 52/1000\n",
      "4208/4208 - 59s - loss: 4.5521 - val_loss: 2.4885 - 59s/epoch - 14ms/step\n",
      "Epoch 53/1000\n",
      "4208/4208 - 58s - loss: 4.3596 - val_loss: 2.4459 - 58s/epoch - 14ms/step\n",
      "Epoch 54/1000\n",
      "4208/4208 - 57s - loss: 4.1684 - val_loss: 2.4457 - 57s/epoch - 14ms/step\n",
      "Epoch 55/1000\n",
      "4208/4208 - 58s - loss: 3.9913 - val_loss: 2.3731 - 58s/epoch - 14ms/step\n",
      "Epoch 56/1000\n",
      "4208/4208 - 58s - loss: 3.8239 - val_loss: 2.3395 - 58s/epoch - 14ms/step\n",
      "Epoch 57/1000\n",
      "4208/4208 - 58s - loss: 3.6617 - val_loss: 2.2782 - 58s/epoch - 14ms/step\n",
      "Epoch 58/1000\n",
      "4208/4208 - 57s - loss: 3.5057 - val_loss: 2.2422 - 57s/epoch - 14ms/step\n",
      "Epoch 59/1000\n",
      "4208/4208 - 58s - loss: 3.3600 - val_loss: 2.1920 - 58s/epoch - 14ms/step\n",
      "Epoch 60/1000\n",
      "4208/4208 - 57s - loss: 3.2237 - val_loss: 2.2182 - 57s/epoch - 14ms/step\n",
      "Epoch 61/1000\n",
      "4208/4208 - 57s - loss: 3.0926 - val_loss: 2.1445 - 57s/epoch - 14ms/step\n",
      "Epoch 62/1000\n",
      "4208/4208 - 56s - loss: 2.9660 - val_loss: 2.1210 - 56s/epoch - 13ms/step\n",
      "Epoch 63/1000\n",
      "4208/4208 - 58s - loss: 2.8477 - val_loss: 2.0797 - 58s/epoch - 14ms/step\n",
      "Epoch 64/1000\n",
      "4208/4208 - 57s - loss: 2.7374 - val_loss: 2.0701 - 57s/epoch - 13ms/step\n",
      "Epoch 65/1000\n",
      "4208/4208 - 57s - loss: 2.6329 - val_loss: 2.0227 - 57s/epoch - 14ms/step\n",
      "Epoch 66/1000\n",
      "4208/4208 - 57s - loss: 2.5331 - val_loss: 1.9854 - 57s/epoch - 14ms/step\n",
      "Epoch 67/1000\n",
      "4208/4208 - 58s - loss: 2.4422 - val_loss: 1.9679 - 58s/epoch - 14ms/step\n",
      "Epoch 68/1000\n",
      "4208/4208 - 57s - loss: 2.3578 - val_loss: 1.9594 - 57s/epoch - 14ms/step\n",
      "Epoch 69/1000\n",
      "4208/4208 - 57s - loss: 2.2773 - val_loss: 1.9320 - 57s/epoch - 13ms/step\n",
      "Epoch 70/1000\n",
      "4208/4208 - 57s - loss: 2.2071 - val_loss: 1.9189 - 57s/epoch - 14ms/step\n",
      "Epoch 71/1000\n",
      "4208/4208 - 58s - loss: 2.1359 - val_loss: 1.9019 - 58s/epoch - 14ms/step\n",
      "Epoch 72/1000\n",
      "4208/4208 - 56s - loss: 2.0688 - val_loss: 1.8923 - 56s/epoch - 13ms/step\n",
      "Epoch 73/1000\n",
      "4208/4208 - 56s - loss: 2.0154 - val_loss: 1.9008 - 56s/epoch - 13ms/step\n",
      "Epoch 74/1000\n",
      "4208/4208 - 58s - loss: 1.9598 - val_loss: 1.8744 - 58s/epoch - 14ms/step\n",
      "Epoch 75/1000\n",
      "4208/4208 - 57s - loss: 1.9171 - val_loss: 1.8789 - 57s/epoch - 14ms/step\n",
      "Epoch 76/1000\n",
      "4208/4208 - 58s - loss: 1.8893 - val_loss: 1.8822 - 58s/epoch - 14ms/step\n",
      "Epoch 77/1000\n",
      "4208/4208 - 57s - loss: 1.8740 - val_loss: 1.8931 - 57s/epoch - 14ms/step\n",
      "Epoch 78/1000\n",
      "4208/4208 - 57s - loss: 1.8673 - val_loss: 1.8979 - 57s/epoch - 14ms/step\n",
      "Epoch 79/1000\n",
      "4208/4208 - 59s - loss: 1.8627 - val_loss: 1.9084 - 59s/epoch - 14ms/step\n",
      "Epoch 80/1000\n",
      "4208/4208 - 58s - loss: 1.8568 - val_loss: 1.8986 - 58s/epoch - 14ms/step\n",
      "Epoch 81/1000\n",
      "4208/4208 - 58s - loss: 1.8459 - val_loss: 1.9049 - 58s/epoch - 14ms/step\n",
      "Epoch 82/1000\n",
      "4208/4208 - 58s - loss: 1.8390 - val_loss: 1.9055 - 58s/epoch - 14ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp_r3rzijx/model/data/model/assets\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/30 04:36:19 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/30 04:36:19 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4208/4208 - 61s - loss: 30.3274 - val_loss: 31.9827 - 61s/epoch - 15ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 60s - loss: 19.7682 - val_loss: 12.0792 - 60s/epoch - 14ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 60s - loss: 17.5916 - val_loss: 5.5596 - 60s/epoch - 14ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 59s - loss: 17.0513 - val_loss: 5.2785 - 59s/epoch - 14ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 59s - loss: 16.7418 - val_loss: 3.9841 - 59s/epoch - 14ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 59s - loss: 16.4294 - val_loss: 4.1055 - 59s/epoch - 14ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 58s - loss: 16.1567 - val_loss: 3.8584 - 58s/epoch - 14ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 59s - loss: 15.8812 - val_loss: 3.9697 - 59s/epoch - 14ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 59s - loss: 15.6089 - val_loss: 3.6745 - 59s/epoch - 14ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 60s - loss: 15.3476 - val_loss: 3.9236 - 60s/epoch - 14ms/step\n",
      "Epoch 11/1000\n",
      "4208/4208 - 59s - loss: 15.0793 - val_loss: 3.8225 - 59s/epoch - 14ms/step\n",
      "Epoch 12/1000\n",
      "4208/4208 - 59s - loss: 14.8011 - val_loss: 3.9443 - 59s/epoch - 14ms/step\n",
      "Epoch 13/1000\n",
      "4208/4208 - 60s - loss: 14.5259 - val_loss: 3.8387 - 60s/epoch - 14ms/step\n",
      "Epoch 14/1000\n",
      "4208/4208 - 60s - loss: 14.2544 - val_loss: 3.7752 - 60s/epoch - 14ms/step\n",
      "Epoch 15/1000\n",
      "4208/4208 - 59s - loss: 13.9860 - val_loss: 3.6930 - 59s/epoch - 14ms/step\n",
      "Epoch 16/1000\n",
      "4208/4208 - 60s - loss: 13.7170 - val_loss: 3.7370 - 60s/epoch - 14ms/step\n",
      "Epoch 17/1000\n",
      "4208/4208 - 60s - loss: 13.4486 - val_loss: 3.7584 - 60s/epoch - 14ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpf09008rp/model/data/model/assets\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/30 04:58:59 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/30 04:58:59 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4208/4208 - 65s - loss: 27.1439 - val_loss: 4.6236 - 65s/epoch - 16ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 64s - loss: 17.8921 - val_loss: 4.1622 - 64s/epoch - 15ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 65s - loss: 17.4471 - val_loss: 4.1686 - 65s/epoch - 15ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 65s - loss: 17.0873 - val_loss: 4.2097 - 65s/epoch - 15ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 64s - loss: 16.8135 - val_loss: 4.4906 - 64s/epoch - 15ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 66s - loss: 16.5478 - val_loss: 4.2882 - 66s/epoch - 16ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 64s - loss: 16.2633 - val_loss: 4.4053 - 64s/epoch - 15ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 63s - loss: 15.9967 - val_loss: 4.4059 - 63s/epoch - 15ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 65s - loss: 15.7297 - val_loss: 4.2730 - 65s/epoch - 15ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 64s - loss: 15.4653 - val_loss: 4.2570 - 64s/epoch - 15ms/step\n",
      "1/1 [==============================] - 1s 980ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpud0ckvu1/model/data/model/assets\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/30 05:15:38 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/30 05:15:38 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4208/4208 - 69s - loss: 38.9545 - val_loss: 6.4766 - 69s/epoch - 16ms/step\n",
      "Epoch 2/1000\n",
      "4208/4208 - 65s - loss: 25.1784 - val_loss: 5.5512 - 65s/epoch - 16ms/step\n",
      "Epoch 3/1000\n",
      "4208/4208 - 65s - loss: 24.2378 - val_loss: 4.9742 - 65s/epoch - 16ms/step\n",
      "Epoch 4/1000\n",
      "4208/4208 - 64s - loss: 23.7437 - val_loss: 5.5982 - 64s/epoch - 15ms/step\n",
      "Epoch 5/1000\n",
      "4208/4208 - 62s - loss: 23.2908 - val_loss: 4.8759 - 62s/epoch - 15ms/step\n",
      "Epoch 6/1000\n",
      "4208/4208 - 64s - loss: 22.8658 - val_loss: 4.5250 - 64s/epoch - 15ms/step\n",
      "Epoch 7/1000\n",
      "4208/4208 - 66s - loss: 22.4555 - val_loss: 4.5804 - 66s/epoch - 16ms/step\n",
      "Epoch 8/1000\n",
      "4208/4208 - 66s - loss: 22.0511 - val_loss: 4.5847 - 66s/epoch - 16ms/step\n",
      "Epoch 9/1000\n",
      "4208/4208 - 67s - loss: 21.6444 - val_loss: 4.6787 - 67s/epoch - 16ms/step\n",
      "Epoch 10/1000\n",
      "4208/4208 - 66s - loss: 21.2552 - val_loss: 4.5875 - 66s/epoch - 16ms/step\n",
      "Epoch 11/1000\n",
      "4208/4208 - 65s - loss: 20.8544 - val_loss: 4.2960 - 65s/epoch - 16ms/step\n",
      "Epoch 12/1000\n",
      "4208/4208 - 65s - loss: 20.4456 - val_loss: 4.4628 - 65s/epoch - 15ms/step\n",
      "Epoch 13/1000\n",
      "4208/4208 - 63s - loss: 20.0571 - val_loss: 4.4059 - 63s/epoch - 15ms/step\n",
      "Epoch 14/1000\n",
      "4208/4208 - 65s - loss: 19.6622 - val_loss: 4.3801 - 65s/epoch - 15ms/step\n",
      "Epoch 15/1000\n",
      "4208/4208 - 65s - loss: 19.2682 - val_loss: 4.3843 - 65s/epoch - 15ms/step\n",
      "Epoch 16/1000\n",
      "4208/4208 - 66s - loss: 18.8673 - val_loss: 4.2855 - 66s/epoch - 16ms/step\n",
      "Epoch 17/1000\n",
      "4208/4208 - 65s - loss: 18.4700 - val_loss: 4.3376 - 65s/epoch - 15ms/step\n",
      "Epoch 18/1000\n",
      "4208/4208 - 64s - loss: 18.0716 - val_loss: 4.3917 - 64s/epoch - 15ms/step\n",
      "Epoch 19/1000\n",
      "4208/4208 - 65s - loss: 17.6821 - val_loss: 4.2489 - 65s/epoch - 15ms/step\n",
      "Epoch 20/1000\n",
      "4208/4208 - 66s - loss: 17.2777 - val_loss: 4.4991 - 66s/epoch - 16ms/step\n",
      "Epoch 21/1000\n",
      "4208/4208 - 64s - loss: 16.8927 - val_loss: 4.0350 - 64s/epoch - 15ms/step\n",
      "Epoch 22/1000\n",
      "4208/4208 - 64s - loss: 16.4973 - val_loss: 3.9913 - 64s/epoch - 15ms/step\n",
      "Epoch 23/1000\n",
      "4208/4208 - 63s - loss: 16.1020 - val_loss: 4.1016 - 63s/epoch - 15ms/step\n",
      "Epoch 24/1000\n",
      "4208/4208 - 65s - loss: 15.7057 - val_loss: 4.0339 - 65s/epoch - 16ms/step\n",
      "Epoch 25/1000\n",
      "4208/4208 - 63s - loss: 15.3125 - val_loss: 3.8981 - 63s/epoch - 15ms/step\n",
      "Epoch 26/1000\n",
      "4208/4208 - 63s - loss: 14.9254 - val_loss: 3.8273 - 63s/epoch - 15ms/step\n",
      "Epoch 27/1000\n",
      "4208/4208 - 62s - loss: 14.5365 - val_loss: 3.8634 - 62s/epoch - 15ms/step\n",
      "Epoch 28/1000\n",
      "4208/4208 - 63s - loss: 14.1387 - val_loss: 3.8080 - 63s/epoch - 15ms/step\n",
      "Epoch 29/1000\n",
      "4208/4208 - 62s - loss: 13.7487 - val_loss: 3.8090 - 62s/epoch - 15ms/step\n",
      "Epoch 30/1000\n",
      "4208/4208 - 63s - loss: 13.3514 - val_loss: 3.4907 - 63s/epoch - 15ms/step\n",
      "Epoch 31/1000\n",
      "4208/4208 - 65s - loss: 12.9543 - val_loss: 3.3411 - 65s/epoch - 15ms/step\n",
      "Epoch 32/1000\n",
      "4208/4208 - 64s - loss: 12.5659 - val_loss: 3.4488 - 64s/epoch - 15ms/step\n",
      "Epoch 33/1000\n",
      "4208/4208 - 64s - loss: 12.1747 - val_loss: 3.4440 - 64s/epoch - 15ms/step\n",
      "Epoch 34/1000\n",
      "4208/4208 - 64s - loss: 11.7852 - val_loss: 3.3546 - 64s/epoch - 15ms/step\n",
      "Epoch 35/1000\n",
      "4208/4208 - 65s - loss: 11.3972 - val_loss: 3.1426 - 65s/epoch - 16ms/step\n",
      "Epoch 36/1000\n",
      "4208/4208 - 66s - loss: 11.0153 - val_loss: 3.3109 - 66s/epoch - 16ms/step\n",
      "Epoch 37/1000\n",
      "4208/4208 - 65s - loss: 10.6249 - val_loss: 3.4365 - 65s/epoch - 16ms/step\n",
      "Epoch 38/1000\n",
      "4208/4208 - 65s - loss: 10.2456 - val_loss: 3.2031 - 65s/epoch - 15ms/step\n",
      "Epoch 39/1000\n",
      "4208/4208 - 66s - loss: 9.8625 - val_loss: 3.2806 - 66s/epoch - 16ms/step\n",
      "Epoch 40/1000\n",
      "4208/4208 - 66s - loss: 9.4827 - val_loss: 3.1278 - 66s/epoch - 16ms/step\n",
      "Epoch 41/1000\n",
      "4208/4208 - 67s - loss: 9.1060 - val_loss: 3.0007 - 67s/epoch - 16ms/step\n",
      "Epoch 42/1000\n",
      "4208/4208 - 68s - loss: 8.7269 - val_loss: 2.9708 - 68s/epoch - 16ms/step\n",
      "Epoch 43/1000\n",
      "4208/4208 - 66s - loss: 8.3591 - val_loss: 3.0509 - 66s/epoch - 16ms/step\n",
      "Epoch 44/1000\n",
      "4208/4208 - 65s - loss: 7.9930 - val_loss: 2.9179 - 65s/epoch - 16ms/step\n",
      "Epoch 45/1000\n",
      "4208/4208 - 66s - loss: 7.6264 - val_loss: 2.8599 - 66s/epoch - 16ms/step\n",
      "Epoch 46/1000\n",
      "4208/4208 - 65s - loss: 7.2668 - val_loss: 2.8211 - 65s/epoch - 15ms/step\n",
      "Epoch 47/1000\n",
      "4208/4208 - 65s - loss: 6.9092 - val_loss: 2.9343 - 65s/epoch - 15ms/step\n",
      "Epoch 48/1000\n",
      "4208/4208 - 66s - loss: 6.5634 - val_loss: 2.8930 - 66s/epoch - 16ms/step\n",
      "Epoch 49/1000\n",
      "4208/4208 - 65s - loss: 6.2268 - val_loss: 2.8521 - 65s/epoch - 15ms/step\n",
      "Epoch 50/1000\n",
      "4208/4208 - 65s - loss: 5.9039 - val_loss: 2.7862 - 65s/epoch - 16ms/step\n",
      "Epoch 51/1000\n",
      "4208/4208 - 65s - loss: 5.5951 - val_loss: 2.7041 - 65s/epoch - 15ms/step\n",
      "Epoch 52/1000\n",
      "4208/4208 - 66s - loss: 5.2963 - val_loss: 2.6487 - 66s/epoch - 16ms/step\n",
      "Epoch 53/1000\n",
      "4208/4208 - 65s - loss: 5.0142 - val_loss: 2.6107 - 65s/epoch - 16ms/step\n",
      "Epoch 54/1000\n",
      "4208/4208 - 64s - loss: 4.7523 - val_loss: 2.6667 - 64s/epoch - 15ms/step\n",
      "Epoch 55/1000\n",
      "4208/4208 - 64s - loss: 4.5042 - val_loss: 2.6167 - 64s/epoch - 15ms/step\n",
      "Epoch 56/1000\n",
      "4208/4208 - 63s - loss: 4.2721 - val_loss: 2.6301 - 63s/epoch - 15ms/step\n",
      "Epoch 57/1000\n",
      "4208/4208 - 62s - loss: 4.0529 - val_loss: 2.5052 - 62s/epoch - 15ms/step\n",
      "Epoch 58/1000\n",
      "4208/4208 - 65s - loss: 3.8374 - val_loss: 2.4093 - 65s/epoch - 15ms/step\n",
      "Epoch 59/1000\n",
      "4208/4208 - 63s - loss: 3.6388 - val_loss: 2.4292 - 63s/epoch - 15ms/step\n",
      "Epoch 60/1000\n",
      "4208/4208 - 66s - loss: 3.4519 - val_loss: 2.3443 - 66s/epoch - 16ms/step\n",
      "Epoch 61/1000\n",
      "4208/4208 - 63s - loss: 3.2776 - val_loss: 2.2893 - 63s/epoch - 15ms/step\n",
      "Epoch 62/1000\n",
      "4208/4208 - 65s - loss: 3.1127 - val_loss: 2.2692 - 65s/epoch - 15ms/step\n",
      "Epoch 63/1000\n",
      "4208/4208 - 65s - loss: 2.9596 - val_loss: 2.2155 - 65s/epoch - 15ms/step\n",
      "Epoch 64/1000\n",
      "4208/4208 - 64s - loss: 2.8147 - val_loss: 2.1673 - 64s/epoch - 15ms/step\n",
      "Epoch 65/1000\n",
      "4208/4208 - 64s - loss: 2.6822 - val_loss: 2.1644 - 64s/epoch - 15ms/step\n",
      "Epoch 66/1000\n",
      "4208/4208 - 64s - loss: 2.5634 - val_loss: 2.1393 - 64s/epoch - 15ms/step\n",
      "Epoch 67/1000\n",
      "4208/4208 - 65s - loss: 2.4574 - val_loss: 2.0881 - 65s/epoch - 15ms/step\n",
      "Epoch 68/1000\n",
      "4208/4208 - 65s - loss: 2.3670 - val_loss: 2.0883 - 65s/epoch - 15ms/step\n",
      "Epoch 69/1000\n",
      "4208/4208 - 66s - loss: 2.2922 - val_loss: 2.0691 - 66s/epoch - 16ms/step\n",
      "Epoch 70/1000\n",
      "4208/4208 - 65s - loss: 2.2368 - val_loss: 2.0387 - 65s/epoch - 15ms/step\n",
      "Epoch 71/1000\n",
      "4208/4208 - 66s - loss: 2.1906 - val_loss: 2.0124 - 66s/epoch - 16ms/step\n",
      "Epoch 72/1000\n",
      "4208/4208 - 65s - loss: 2.1655 - val_loss: 1.9920 - 65s/epoch - 16ms/step\n",
      "Epoch 73/1000\n",
      "4208/4208 - 65s - loss: 2.1603 - val_loss: 1.9846 - 65s/epoch - 15ms/step\n",
      "Epoch 74/1000\n",
      "4208/4208 - 67s - loss: 2.1375 - val_loss: 1.9497 - 67s/epoch - 16ms/step\n",
      "Epoch 75/1000\n",
      "4208/4208 - 65s - loss: 2.1276 - val_loss: 1.9273 - 65s/epoch - 16ms/step\n",
      "Epoch 76/1000\n",
      "4208/4208 - 64s - loss: 2.1190 - val_loss: 1.9233 - 64s/epoch - 15ms/step\n",
      "Epoch 77/1000\n",
      "4208/4208 - 64s - loss: 2.1180 - val_loss: 1.9378 - 64s/epoch - 15ms/step\n",
      "Epoch 78/1000\n",
      "4208/4208 - 64s - loss: 2.1096 - val_loss: 1.9295 - 64s/epoch - 15ms/step\n",
      "Epoch 79/1000\n",
      "4208/4208 - 64s - loss: 2.1029 - val_loss: 1.9179 - 64s/epoch - 15ms/step\n",
      "Epoch 80/1000\n",
      "4208/4208 - 63s - loss: 2.0958 - val_loss: 1.9075 - 63s/epoch - 15ms/step\n",
      "Epoch 81/1000\n",
      "4208/4208 - 65s - loss: 2.1084 - val_loss: 1.9409 - 65s/epoch - 15ms/step\n",
      "Epoch 82/1000\n",
      "4208/4208 - 63s - loss: 2.0894 - val_loss: 1.9092 - 63s/epoch - 15ms/step\n",
      "Epoch 83/1000\n",
      "4208/4208 - 65s - loss: 2.0840 - val_loss: 1.8968 - 65s/epoch - 15ms/step\n",
      "Epoch 84/1000\n",
      "4208/4208 - 65s - loss: 2.0814 - val_loss: 1.9215 - 65s/epoch - 15ms/step\n",
      "Epoch 85/1000\n",
      "4208/4208 - 66s - loss: 2.0784 - val_loss: 1.9154 - 66s/epoch - 16ms/step\n",
      "Epoch 86/1000\n",
      "4208/4208 - 64s - loss: 2.0802 - val_loss: 1.9123 - 64s/epoch - 15ms/step\n",
      "Epoch 87/1000\n",
      "4208/4208 - 65s - loss: 2.0709 - val_loss: 1.9208 - 65s/epoch - 15ms/step\n",
      "Epoch 88/1000\n",
      "4208/4208 - 65s - loss: 2.0669 - val_loss: 1.9043 - 65s/epoch - 16ms/step\n",
      "Epoch 89/1000\n",
      "4208/4208 - 67s - loss: 2.0647 - val_loss: 1.8917 - 67s/epoch - 16ms/step\n",
      "Epoch 90/1000\n"
     ]
    }
   ],
   "source": [
    "# AE\n",
    "\n",
    "for _ in range(100):\n",
    "    config = {'shape': 1957,\n",
    "              'output': 72,\n",
    "              'dropout': np.random.choice([0.01, 0.1, 0.2, 0.5]),\n",
    "              'num_layers': np.random.choice([3, 4, 5, 10]),\n",
    "              'num_neurons': np.random.choice([16, 32, 64, 128]),\n",
    "              'activation': np.random.choice(['gelu', 'gelu', 'relu'])}\n",
    "\n",
    "    model = build_ae_cpl_model(config)\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        model_checkpoint_callback = ModelCheckpoint(\n",
    "            filepath=f\"models/model_{run.info.run_id}.keras\",\n",
    "            save_weights_only=False,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "        callback = EarlyStopping(monitor='val_loss', patience=8, verbose=0)\n",
    "        mlflow.log_param(\"Autoencoder\", True)\n",
    "        mlflow.log_param(\"CPL\", True)\n",
    "\n",
    "        history = model.fit(x_train, y_train, epochs=1000, batch_size=2000,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            verbose=2,\n",
    "                            callbacks=[callback, model_checkpoint_callback])\n",
    "        \n",
    "        evaled_test = model.evaluate(x_train, y_train, batch_size=100, verbose=0)\n",
    "        evaled_train = model.evaluate(x_test, y_test, batch_size=100, verbose=0)\n",
    "\n",
    "        mlflow.log_metric(\"test_loss\", evaled_test)\n",
    "        mlflow.log_metric(\"train_loss\", evaled_train)\n",
    "\n",
    "        plt.plot(history.history[\"loss\"])\n",
    "        plt.plot(history.history[\"val_loss\"])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plotname = f\"plots/loss_plot_{run.info.run_id}.png\"\n",
    "        plt.savefig(plotname)\n",
    "        plt.close()\n",
    "\n",
    "        # Log the plot as an artifact\n",
    "        mlflow.log_artifact(plotname)\n",
    "        #model_path = \"modles\"\n",
    "        #mlflow.tensorflow.log_model(tf_saved_model_dir=model_path, artifact_path=\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04622045-1537-4c5d-ab12-9eab1cd35c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regs\n",
    "\n",
    "for _ in range(100):\n",
    "    config = {'shape': 1957,\n",
    "              'output': 72,\n",
    "              'dropout': np.random.choice([0.01, 0.1, 0.2, 0.5]),\n",
    "              'num_layers': np.random.choice([3, 4, 5, 10]),\n",
    "              'num_neurons': np.random.choice([16, 32, 64, 128]),\n",
    "              'activation': np.random.choice(['gelu', 'gelu', 'relu'])}\n",
    "    \n",
    "    model = build_ae_model(config)\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        model_checkpoint_callback = ModelCheckpoint(\n",
    "            filepath=f\"models/model_{run.info.run_id}.keras\",\n",
    "            save_weights_only=False,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "        callback = EarlyStopping(monitor='val_loss', patience=8, verbose=0)\n",
    "        mlflow.log_param(\"Autoencoder\", True)\n",
    "        mlflow.log_param(\"CPL\", False)\n",
    "\n",
    "        history = model.fit(x_train, y_train, epochs=1000, batch_size=2000,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            verbose=2,\n",
    "                            callbacks=[callback, model_checkpoint_callback])\n",
    "        \n",
    "        evaled_test = model.evaluate(x_train, y_train, batch_size=100, verbose=0)\n",
    "        evaled_train = model.evaluate(x_test, y_test, batch_size=100, verbose=0)\n",
    "\n",
    "        mlflow.log_metric(\"test_loss\", evaled_test)\n",
    "        mlflow.log_metric(\"train_loss\", evaled_train)\n",
    "\n",
    "        plt.plot(history.history[\"loss\"])\n",
    "        plt.plot(history.history[\"val_loss\"])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plotname = f\"plots/loss_plot_{run.info.run_id}.png\"\n",
    "        plt.savefig(plotname)\n",
    "        plt.close()\n",
    "\n",
    "        # Log the plot as an artifact\n",
    "        mlflow.log_artifact(plotname)\n",
    "        #model_path = \"modles\"\n",
    "        #mlflow.tensorflow.log_model(tf_saved_model_dir=model_path, artifact_path=\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d5b96-20c7-4802-939d-2135ffabe8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
