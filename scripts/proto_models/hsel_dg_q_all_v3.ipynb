{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "008ce633-8ecc-4d52-a254-a0784d743198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 08:32:22.213689: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-05 08:32:22.320068: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import isfile\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "#os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L1, L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3ea1be-bfd5-4112-989a-8d9bd16bcb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 08:32:27 INFO mlflow.tracking.fluent: Experiment with name 'HSEL fixed q Reduced all levels new' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///data/nature_run/work/src/mlruns/931405904624679090', creation_time=1725539547129, experiment_id='931405904624679090', last_update_time=1725539547129, lifecycle_stage='active', name='HSEL fixed q Reduced all levels new', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.tensorflow.autolog()\n",
    "mlflow.set_experiment(\"HSEL fixed q Reduced all levels new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbcb607-662b-47f4-b6a7-72761b09e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = ['/data/nature_run/fulldays_reduced/all_20060803.npz', '/data/nature_run/fulldays_reduced/all_20060815.npz', '/data/nature_run/fulldays_reduced/all_20060915.npz']\n",
    "#test = ['/data/nature_run/fulldays_reduced/all_20060315.npz', '/data/nature_run/fulldays_reduced/all_20060515.npz', '/data/nature_run/fulldays_reduced/all_20060615.npz', '/data/nature_run/fulldays_reduced/all_20060715.npz', '/data/nature_run/fulldays_reduced/all_20061015.npz', '/data/nature_run/fulldays_reduced/all_20061115.npz', '/data/nature_run/fulldays_reduced/all_20061215.npz']\n",
    "\n",
    "#print(len(train), len(test), len(train)+ len(test)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6bafc-be5f-4bcb-9bd9-16ea3147f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"/data/nature_run/fulldays_reduced_evenmore/*.npz\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b48a92-7531-44b6-8461-5c367878d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filelist):\n",
    "    hsel_val = []\n",
    "    scalar_val = []\n",
    "    table_val = []\n",
    "\n",
    "    for i in filelist:\n",
    "        # Keys: 'mh', 'hsel', 'scalar', 'table']\n",
    "        tmp = np.load(i)\n",
    "        hsel_val.append(tmp[\"hsel\"])\n",
    "        scalar_val.append(tmp[\"scalar\"])\n",
    "        table_val.append(tmp[\"table\"])\n",
    "\n",
    "    return np.vstack(hsel_val), np.vstack(scalar_val), np.vstack(table_val)\n",
    "\n",
    "\n",
    "train = ['/data/nature_run/fulldays_reduced_evenmore/all_20060815.npz',\n",
    "         '/data/nature_run/fulldays_reduced_evenmore/all_20060915.npz',\n",
    "         '/data/nature_run/fulldays_reduced_evenmore/all_20061015.npz',\n",
    "         '/data/nature_run/fulldays_reduced_evenmore/all_20060515.npz',\n",
    "         '/data/nature_run/fulldays_reduced_evenmore/all_20060715.npz',\n",
    "         '/data/nature_run/fulldays_reduced_evenmore/all_20061115.npz',\n",
    "         '/data/nature_run/fulldays_reduced_evenmore/all_20060315.npz', \n",
    "         '/data/nature_run/fulldays_reduced_evenmore/all_20061215.npz',\n",
    "         '/data/nature_run/fulldays_reduced_evenmore/all_20060615.npz']\n",
    "\n",
    "test = ['/data/nature_run/fulldays_reduced_evenmore/all_20060803.npz']\n",
    "\n",
    "validation = ['/data/nature_run/fulldays_reduced_evenmore/all_20060803.npz']\n",
    "\n",
    "hsel_train, scalar_train, table_train = get_data(train)\n",
    "hsel_test, scalar_test, table_test = get_data(test)\n",
    "hsel_val, scalar_val, table_val = get_data(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9fdb1e-437c-479f-9e12-88d4898cfcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AE\n",
    "\n",
    "def dataload(instr):\n",
    "\n",
    "    train = [\"/data/nature_run/fulldays_reduced_evenmore/20060615/mh.npy\",\n",
    "             \"/data/nature_run/fulldays_reduced_evenmore/20061215/mh.npy\",\n",
    "             \"/data/nature_run/fulldays_reduced_evenmore/20060515/mh.npy\",\n",
    "             \"/data/nature_run/fulldays_reduced_evenmore/20060815/mh.npy\",\n",
    "             \"/data/nature_run/fulldays_reduced_evenmore/20060915/mh.npy\",\n",
    "             \"/data/nature_run/fulldays_reduced_evenmore/20060715/mh.npy\",\n",
    "             \"/data/nature_run/fulldays_reduced_evenmore/20061015/mh.npy\",\n",
    "             \"/data/nature_run/fulldays_reduced_evenmore/20060315/mh.npy\",\n",
    "             \"/data/nature_run/fulldays_reduced_evenmore/20061115/mh.npy\"]\n",
    "\n",
    "    test = [\"/data/nature_run/fulldays_reduced_evenmore/20060803/mh.npy\"]\n",
    "\n",
    "    if instr == \"hsel\":\n",
    "        train = [x.replace(\"mh\", instr) for x in train]\n",
    "        test = [x.replace(\"mh\", instr) for x in test]\n",
    "\n",
    "    dat_train = []\n",
    "    scalar_train = []\n",
    "    table_train = []\n",
    "    \n",
    "    dat_test = []\n",
    "    scalar_test = []\n",
    "    table_test = []\n",
    "    \n",
    "    for i in train:\n",
    "        # Keys: 'mh', 'hsel', 'scalar', 'table']\n",
    "        tmp = np.load(i)\n",
    "        dat_train.append(tmp)\n",
    "        \n",
    "        tmp = np.load(i.replace(instr, \"table\"))\n",
    "        table_train.append(tmp)\n",
    "    \n",
    "        tmp = np.load(i.replace(instr, \"scalar\"))\n",
    "        scalar_train.append(tmp)\n",
    "    \n",
    "    \n",
    "    dat_train = np.vstack(dat_train)\n",
    "    table_train = np.vstack(table_train)\n",
    "    scalar_train = np.vstack(scalar_train)\n",
    "        \n",
    "    for i in test:\n",
    "        # Keys: 'mh', 'hsel', 'scalar', 'table']\n",
    "        tmp = np.load(i)\n",
    "        dat_test.append(tmp)\n",
    "        \n",
    "        tmp = np.load(i.replace(instr, \"table\"))\n",
    "        table_test.append(tmp)\n",
    "    \n",
    "        tmp = np.load(i.replace(instr, \"scalar\"))\n",
    "        scalar_test.append(tmp)\n",
    "    \n",
    "    \n",
    "    dat_test = np.vstack(dat_test)\n",
    "    table_test = np.vstack(table_test)\n",
    "    scalar_test = np.vstack(scalar_test)\n",
    "    \n",
    "    return dat_train, table_train, scalar_train, dat_test, table_test, scalar_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b383e5-cba2-4bd6-b24d-ab886f716a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsel_train, table_train, scalar_train, hsel_test, table_test, scalar_test = dataload(\"hsel\")\n",
    "hsel_val = hsel_test.copy()\n",
    "table_val = table_test.copy()\n",
    "scalar_val = scalar_test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc1aef-d6f3-4fcd-a3d9-ed736e5d6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spress_test = scalar_test[:, 3].reshape(-1, 1)\n",
    "# spress_train = scalar_train[:, 3].reshape(-1, 1)\n",
    "# spress_val = scalar_val[:, 3].reshape(-1, 1)\n",
    "\n",
    "\n",
    "# spress_test\n",
    "# spress_train\n",
    "# spress_val\n",
    "\n",
    "# q_train = table_train[:, :, 1]\n",
    "# q_test = table_test[:, :, 1]\n",
    "# q_val = table_val[:, :, 1]\n",
    "\n",
    "# mins = np.min(hsel_val, axis=0)\n",
    "# maxs = np.max(hsel_val, axis=0)\n",
    "# hsel_train = (hsel_val - mins)/(maxs - mins)\n",
    "# np.savez(\"hsel_val_scalar.npz\", mins=mins, maxs=maxs)\n",
    "\n",
    "# mins = np.min(hsel_train, axis=0)\n",
    "# maxs = np.max(hsel_train, axis=0)\n",
    "# hsel_train = (hsel_train - mins)/(maxs - mins)\n",
    "# np.savez(\"hsel_train_scalar.npz\", mins=mins, maxs=maxs)\n",
    "\n",
    "# mins = np.min(hsel_test, axis=0)\n",
    "# maxs = np.max(hsel_test, axis=0)\n",
    "# hsel_test = (hsel_test - mins)/(maxs - mins)\n",
    "# np.savez(\"hsel_test_scalar.npz\", mins=mins, maxs=maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2751b636-5210-4f12-8989-d3f9b0a29eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 08:51:50.321848: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-05 08:51:50.852603: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-09-05 08:51:50.853454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31017 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1b:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "# What should we do\n",
    "# 1) Only use train scalars\n",
    "# 2) Scale seperateley\n",
    "\n",
    "\n",
    "# Presssure\n",
    "spress_test = scalar_test[:, 3].reshape(-1, 1)\n",
    "spress_train = scalar_train[:, 3].reshape(-1, 1)\n",
    "spress_val = scalar_val[:, 3].reshape(-1, 1)\n",
    "\n",
    "mins = np.min(spress_train, axis=0)\n",
    "maxs = np.max(spress_train, axis=0)\n",
    "np.savez(\"spress_scalar.npz\", mins=mins, maxs=maxs)\n",
    "spress_test = (spress_test - mins)/(maxs - mins)\n",
    "spress_train = (spress_train - mins)/(maxs - mins)\n",
    "spress_val = (spress_val - mins)/(maxs - mins)\n",
    "\n",
    "\n",
    "# Labels\n",
    "q_train = table_train[:, :, 2]\n",
    "q_test = table_test[:, :, 2]\n",
    "q_val = table_val[:, :, 2]\n",
    "\n",
    "\n",
    "# Spectra\n",
    "\n",
    "#mins = np.min(hsel_train, axis=0)\n",
    "#maxs = np.max(hsel_train, axis=0)\n",
    "\n",
    "s_factors = np.load(\"minimac_scaling_factors_hsel.npz\")\n",
    "mins = s_factors['mins']\n",
    "maxs = s_factors['maxs']\n",
    "\n",
    "\n",
    "hsel_train = np.nan_to_num(hsel_train)\n",
    "hsel_test = np.nan_to_num(hsel_test)\n",
    "hsel_val = np.nan_to_num(hsel_val)\n",
    "\n",
    "hsel_train = (hsel_train - mins)/(maxs - mins)\n",
    "hsel_test = (hsel_test - mins)/(maxs - mins)\n",
    "hsel_val = (hsel_val - mins)/(maxs - mins)\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    # train\n",
    "    x_train = {'rad': tf.convert_to_tensor(hsel_train, np.float32),\n",
    "               'spress': tf.convert_to_tensor(spress_train, np.float32)}\n",
    "    del hsel_train\n",
    "    del spress_train\n",
    "    \n",
    "    y_train =  tf.convert_to_tensor(q_train, np.float32)\n",
    "    del q_train\n",
    "\n",
    "    \n",
    "    # Val\n",
    "    x_val = {'rad': tf.convert_to_tensor(hsel_val, np.float32),\n",
    "             'spress': tf.convert_to_tensor(spress_val, np.float32)}\n",
    "    del hsel_val\n",
    "    del spress_val\n",
    "    \n",
    "    y_val =  tf.convert_to_tensor(q_val, np.float32)\n",
    "    del q_val\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    x_test = {'rad': tf.convert_to_tensor(hsel_test, np.float32),\n",
    "              'spress': tf.convert_to_tensor(spress_test, np.float32)}\n",
    "    del hsel_test\n",
    "    del spress_test\n",
    "    \n",
    "    y_test =  tf.convert_to_tensor(q_test, np.float32)\n",
    "    del q_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66848f44-450d-4748-b044-1d8a1da680a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_test)\n",
    "# print(x_val)\n",
    "# print(y_train.shape)\n",
    "#a = np.argwhere(np.isnan(x_train['rad']))\n",
    "#print(np.sum(np.isnan(x_train['rad'])))\n",
    "#np.nan_to_num(x_train['rad'], copy=False)\n",
    "#print(np.sum(np.isnan(x_train['rad'])))\n",
    "#print(a)\n",
    "#print(len(a))\n",
    "\n",
    "np.sum(np.isnan(x_train['rad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59fdbe91-dd90-46b1-b519-60d0fd1da662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"Temp\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " rad (InputLayer)               [(None, 1957)]       0           []                               \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 64)           275468      ['rad[0][0]']                    \n",
      "                                                                                                  \n",
      " spress (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 65)           0           ['model_1[0][0]',                \n",
      "                                                                  'spress[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           4224        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           4160        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           4160        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           4160        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " Temp (Dense)                   (None, 72)           4680        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 296,852\n",
      "Trainable params: 21,384\n",
      "Non-trainable params: 275,468\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_ae_model(config):\n",
    "    # ATMS 22\n",
    "    mh = Input(shape=(config[\"shape\"],), name=\"rad\")\n",
    "    spress = Input(shape=(1,), name=\"spress\")\n",
    "    # [ha, hb, hc, hd, hw, mh]\n",
    "\n",
    "    encoder = load_model(\"encoder_3_mae.keras\")\n",
    "    encoder.trainable = False\n",
    "    #for layer in encoder.layers:\n",
    "    #    layer.trainable = False\n",
    "\n",
    "    \n",
    "    enc = encoder(mh)\n",
    "    x = Concatenate()([enc, spress])\n",
    "    for i in range(config[\"num_layers\"]):\n",
    "        x =  Dropout(config[\"dropout\"])(Dense(config[\"num_neurons\"], \n",
    "                                             activation=config[\"activation\"])(x))\n",
    "    outputs = Dense(config['output'], name=\"Temp\")(x)\n",
    "\n",
    "    model = Model(inputs=[mh, spress], outputs=outputs, name=\"Temp\")\n",
    "    model.compile(optimizer=\"adam\", loss='mae')\n",
    "\n",
    "    return model\n",
    "\n",
    "config = {'shape': 1957,\n",
    "          'output': 72,\n",
    "          'dropout': np.random.choice([0.01, 0.1, 0.2, 0.5]),\n",
    "          'num_layers': np.random.choice([3, 4, 5, 10]),\n",
    "          'num_neurons': np.random.choice([16, 32, 64, 128]),\n",
    "          'activation': np.random.choice(['gelu', 'gelu', 'relu'])}\n",
    "\n",
    "model = build_ae_model(config)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137680e3-1dc6-4550-8561-a37c87676066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Temp\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " rad (InputLayer)               [(None, 1957)]       0           []                               \n",
      "                                                                                                  \n",
      " spress (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 1958)         0           ['rad[0][0]',                    \n",
      "                                                                  'spress[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           125376      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           4160        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 64)           0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           4160        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 64)           0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64)           4160        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64)           0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 64)           4160        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 64)           0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " Temp (Dense)                   (None, 72)           4680        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 146,696\n",
      "Trainable params: 146,696\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(config):\n",
    "    # ATMS 22\n",
    "    mh = Input(shape=(config[\"shape\"],), name=\"rad\")\n",
    "    spress = Input(shape=(1,), name=\"spress\")\n",
    "    # [ha, hb, hc, hd, hw, mh]\n",
    "    \n",
    "    x = Concatenate()([mh, spress])\n",
    "    for i in range(config[\"num_layers\"]):\n",
    "        x =  Dropout(config[\"dropout\"])(Dense(config[\"num_neurons\"], \n",
    "                                             activation=config[\"activation\"])(x))\n",
    "    outputs = Dense(config['output'], name=\"Temp\")(x)\n",
    "\n",
    "    model = Model(inputs=[mh, spress], outputs=outputs, name=\"Temp\")\n",
    "    model.compile(optimizer=\"adam\", loss='mae')\n",
    "\n",
    "    return model\n",
    "\n",
    "config = {'shape': 1957,\n",
    "          'output': 72,\n",
    "          'dropout': np.random.choice([0.01, 0.1, 0.2, 0.5]),\n",
    "          'num_layers': np.random.choice([3, 4, 5, 10]),\n",
    "          'num_neurons': np.random.choice([16, 32, 64, 128]),\n",
    "          'activation': np.random.choice(['gelu', 'gelu', 'relu'])}\n",
    "\n",
    "model = build_model(config)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3081d9a-e65b-44d5-9c98-9558667c04ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DING\n"
     ]
    }
   ],
   "source": [
    "print(\"DING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b96d48-639c-4f0f-bf07-aa7d69d40e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b849f517-93eb-4370-bff5-0d7e79fc93cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 08:53:52 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/05 08:53:52 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 51s 11ms/step - loss: 9.1588e-04 - val_loss: 4.1626e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.5243e-04 - val_loss: 4.7280e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.2508e-04 - val_loss: 4.0776e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1357e-04 - val_loss: 4.0473e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.0659e-04 - val_loss: 4.1138e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.0217e-04 - val_loss: 4.0087e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 3.9852e-04 - val_loss: 3.7526e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.9809e-04 - val_loss: 4.0884e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 3.9430e-04 - val_loss: 4.0198e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 3.9305e-04 - val_loss: 3.9115e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 3.9012e-04 - val_loss: 3.7965e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 3.8727e-04 - val_loss: 3.9506e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 3.8604e-04 - val_loss: 3.8394e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 3.8549e-04 - val_loss: 3.7671e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 3.8397e-04 - val_loss: 3.6234e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 3.8310e-04 - val_loss: 4.0135e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.8192e-04 - val_loss: 3.8755e-04\n",
      "Epoch 18/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.8045e-04 - val_loss: 3.9047e-04\n",
      "Epoch 19/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.7959e-04 - val_loss: 3.7959e-04\n",
      "Epoch 20/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.7796e-04 - val_loss: 3.8506e-04\n",
      "Epoch 21/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.7768e-04 - val_loss: 4.1945e-04\n",
      "Epoch 22/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.7663e-04 - val_loss: 3.7277e-04\n",
      "Epoch 23/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.7549e-04 - val_loss: 3.8549e-04\n",
      "Epoch 24/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 3.7534e-04 - val_loss: 3.6504e-04\n",
      "Epoch 25/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.7471e-04 - val_loss: 3.9562e-04\n",
      "Epoch 26/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.7375e-04 - val_loss: 4.1274e-04\n",
      "Epoch 27/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.7468e-04 - val_loss: 4.0991e-04\n",
      "Epoch 27: early stopping\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpmvt50iza/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 09:16:39 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/data/jmackin1/miniconda/envs/tf-gpu/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87428/87428 [==============================] - 217s 2ms/step - loss: 3.9407e-04\n",
      "9698/9698 [==============================] - 25s 3ms/step - loss: 4.0991e-04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 09:20:41 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/05 09:20:41 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 0.0011 - val_loss: 8.7853e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8575e-04 - val_loss: 8.9107e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8509e-04 - val_loss: 8.9134e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8538e-04 - val_loss: 8.8930e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8503e-04 - val_loss: 8.8974e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8549e-04 - val_loss: 8.8560e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8500e-04 - val_loss: 8.8574e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 8.8532e-04 - val_loss: 8.9093e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8494e-04 - val_loss: 8.9504e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8500e-04 - val_loss: 8.9313e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8504e-04 - val_loss: 8.9303e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 8.8544e-04 - val_loss: 8.8693e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8534e-04 - val_loss: 8.8289e-04\n",
      "Epoch 13: early stopping\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp2whvt7t5/model/data/model/assets\n",
      "87428/87428 [==============================] - 230s 3ms/step - loss: 8.7950e-04\n",
      "9698/9698 [==============================] - 25s 3ms/step - loss: 8.8289e-04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 09:36:34 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/05 09:36:34 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 0.0013 - val_loss: 9.0932e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8837e-04 - val_loss: 8.8787e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8775e-04 - val_loss: 8.9266e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8784e-04 - val_loss: 8.9146e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.8819e-04 - val_loss: 8.8612e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8788e-04 - val_loss: 8.8925e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8792e-04 - val_loss: 8.9218e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8811e-04 - val_loss: 8.8956e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8790e-04 - val_loss: 9.0078e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8792e-04 - val_loss: 8.9656e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8819e-04 - val_loss: 9.0201e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8788e-04 - val_loss: 8.9596e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8808e-04 - val_loss: 8.8649e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 8.8793e-04 - val_loss: 8.9701e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8807e-04 - val_loss: 8.9885e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8786e-04 - val_loss: 8.9171e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.8787e-04 - val_loss: 8.9699e-04\n",
      "Epoch 17: early stopping\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpf5erwlpy/model/data/model/assets\n",
      "87428/87428 [==============================] - 236s 3ms/step - loss: 8.9391e-04\n",
      "9698/9698 [==============================] - 26s 3ms/step - loss: 8.9699e-04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 09:55:37 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/05 09:55:37 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 5.8829e-04 - val_loss: 3.0895e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 3.0712e-04 - val_loss: 2.9621e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.9121e-04 - val_loss: 2.9392e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.8366e-04 - val_loss: 2.8647e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 2.8025e-04 - val_loss: 2.8330e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7970e-04 - val_loss: 2.8388e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7779e-04 - val_loss: 2.8252e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 2.7613e-04 - val_loss: 2.8286e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7550e-04 - val_loss: 2.7814e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7583e-04 - val_loss: 2.7914e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 2.7587e-04 - val_loss: 2.8605e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7392e-04 - val_loss: 2.7570e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7225e-04 - val_loss: 2.7725e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7243e-04 - val_loss: 2.8674e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7358e-04 - val_loss: 2.8054e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 2.7324e-04 - val_loss: 2.8125e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 2.7141e-04 - val_loss: 2.7847e-04\n",
      "Epoch 18/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7208e-04 - val_loss: 2.8382e-04\n",
      "Epoch 19/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7088e-04 - val_loss: 2.8203e-04\n",
      "Epoch 20/1000\n",
      "4372/4372 [==============================] - 50s 12ms/step - loss: 2.7059e-04 - val_loss: 2.7520e-04\n",
      "Epoch 21/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7170e-04 - val_loss: 2.7397e-04\n",
      "Epoch 22/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7129e-04 - val_loss: 2.8402e-04\n",
      "Epoch 23/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7073e-04 - val_loss: 2.8111e-04\n",
      "Epoch 24/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.6985e-04 - val_loss: 2.7998e-04\n",
      "Epoch 25/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 2.6912e-04 - val_loss: 2.7453e-04\n",
      "Epoch 26/1000\n",
      "4372/4372 [==============================] - 50s 12ms/step - loss: 2.7016e-04 - val_loss: 2.8182e-04\n",
      "Epoch 27/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.7072e-04 - val_loss: 2.8352e-04\n",
      "Epoch 28/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.6998e-04 - val_loss: 2.8412e-04\n",
      "Epoch 29/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.6904e-04 - val_loss: 2.7475e-04\n",
      "Epoch 30/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.6890e-04 - val_loss: 2.7719e-04\n",
      "Epoch 31/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 2.6944e-04 - val_loss: 2.8081e-04\n",
      "Epoch 32/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.6841e-04 - val_loss: 2.7572e-04\n",
      "Epoch 33/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.6846e-04 - val_loss: 2.7547e-04\n",
      "Epoch 33: early stopping\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpyvaxvwx3/model/data/model/assets\n",
      "87428/87428 [==============================] - 245s 3ms/step - loss: 2.5678e-04\n",
      "9698/9698 [==============================] - 29s 3ms/step - loss: 2.7548e-04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 10:29:01 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/05 10:29:01 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 63s 14ms/step - loss: 3.5542e-04 - val_loss: 2.8779e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 60s 14ms/step - loss: 2.9584e-04 - val_loss: 2.9783e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.8785e-04 - val_loss: 2.9331e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.8621e-04 - val_loss: 2.8648e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.8271e-04 - val_loss: 2.8036e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 60s 14ms/step - loss: 2.8017e-04 - val_loss: 2.8283e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.7770e-04 - val_loss: 2.7062e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.7723e-04 - val_loss: 2.8127e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.7557e-04 - val_loss: 2.9070e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.7398e-04 - val_loss: 2.8430e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.7385e-04 - val_loss: 2.7595e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.7323e-04 - val_loss: 2.8315e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.7268e-04 - val_loss: 2.7169e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 60s 14ms/step - loss: 2.7096e-04 - val_loss: 2.8533e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 60s 14ms/step - loss: 2.7094e-04 - val_loss: 2.8037e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 60s 14ms/step - loss: 2.6978e-04 - val_loss: 2.8598e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.6825e-04 - val_loss: 2.9102e-04\n",
      "Epoch 18/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.6999e-04 - val_loss: 2.7991e-04\n",
      "Epoch 19/1000\n",
      "4372/4372 [==============================] - 60s 14ms/step - loss: 2.6968e-04 - val_loss: 2.9082e-04\n",
      "Epoch 19: early stopping\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb6d34e27a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpat16k3r7/model/data/model/assets\n",
      "87428/87428 [==============================] - 307s 4ms/step - loss: 2.6206e-04\n",
      "9698/9698 [==============================] - 33s 3ms/step - loss: 2.9082e-04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 10:54:41 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/05 10:54:41 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 65s 14ms/step - loss: 3.9586e-04 - val_loss: 3.3269e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 3.2314e-04 - val_loss: 3.0300e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 3.1448e-04 - val_loss: 3.0224e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 3.1167e-04 - val_loss: 2.9519e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 3.0625e-04 - val_loss: 2.8961e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 3.0516e-04 - val_loss: 2.9219e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 3.0024e-04 - val_loss: 2.9498e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 3.0166e-04 - val_loss: 2.8918e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 60s 14ms/step - loss: 2.9968e-04 - val_loss: 2.8775e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.9862e-04 - val_loss: 2.9205e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 63s 14ms/step - loss: 2.9753e-04 - val_loss: 2.9550e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.9741e-04 - val_loss: 2.9691e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.9581e-04 - val_loss: 2.9002e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 60s 14ms/step - loss: 2.9376e-04 - val_loss: 2.9900e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 63s 14ms/step - loss: 2.9610e-04 - val_loss: 2.8728e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 63s 14ms/step - loss: 2.9370e-04 - val_loss: 2.8986e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 64s 15ms/step - loss: 2.9237e-04 - val_loss: 2.9453e-04\n",
      "Epoch 18/1000\n",
      "4372/4372 [==============================] - 65s 15ms/step - loss: 2.9381e-04 - val_loss: 2.9560e-04\n",
      "Epoch 19/1000\n",
      "4372/4372 [==============================] - 65s 15ms/step - loss: 2.9197e-04 - val_loss: 2.8615e-04\n",
      "Epoch 20/1000\n",
      "4372/4372 [==============================] - 65s 15ms/step - loss: 2.9026e-04 - val_loss: 2.9457e-04\n",
      "Epoch 21/1000\n",
      "4372/4372 [==============================] - 64s 15ms/step - loss: 2.9265e-04 - val_loss: 2.9127e-04\n",
      "Epoch 22/1000\n",
      "4372/4372 [==============================] - 64s 15ms/step - loss: 2.8994e-04 - val_loss: 2.9743e-04\n",
      "Epoch 23/1000\n",
      "4372/4372 [==============================] - 65s 15ms/step - loss: 2.9037e-04 - val_loss: 2.8944e-04\n",
      "Epoch 24/1000\n",
      "4372/4372 [==============================] - 65s 15ms/step - loss: 2.9114e-04 - val_loss: 2.8119e-04\n",
      "Epoch 25/1000\n",
      "4372/4372 [==============================] - 64s 15ms/step - loss: 2.9013e-04 - val_loss: 2.9531e-04\n",
      "Epoch 26/1000\n",
      "4372/4372 [==============================] - 65s 15ms/step - loss: 2.8970e-04 - val_loss: 2.9708e-04\n",
      "Epoch 27/1000\n",
      "4372/4372 [==============================] - 64s 15ms/step - loss: 2.9230e-04 - val_loss: 2.9581e-04\n",
      "Epoch 28/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.9038e-04 - val_loss: 2.9582e-04\n",
      "Epoch 29/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.8833e-04 - val_loss: 2.9058e-04\n",
      "Epoch 30/1000\n",
      "4372/4372 [==============================] - 63s 14ms/step - loss: 2.9008e-04 - val_loss: 3.0113e-04\n",
      "Epoch 31/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.9099e-04 - val_loss: 2.9432e-04\n",
      "Epoch 32/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.8889e-04 - val_loss: 2.9341e-04\n",
      "Epoch 33/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.9195e-04 - val_loss: 2.9179e-04\n",
      "Epoch 34/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.8924e-04 - val_loss: 2.9516e-04\n",
      "Epoch 35/1000\n",
      "4372/4372 [==============================] - 61s 14ms/step - loss: 2.9187e-04 - val_loss: 2.9159e-04\n",
      "Epoch 36/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.8748e-04 - val_loss: 2.8356e-04\n",
      "Epoch 36: early stopping\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb6d3600040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp3j5mpn7o/model/data/model/assets\n",
      "87428/87428 [==============================] - 308s 4ms/step - loss: 2.6983e-04\n",
      "9698/9698 [==============================] - 34s 3ms/step - loss: 2.8356e-04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 11:38:48 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/05 11:38:48 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 9.3571e-04 - val_loss: 4.1293e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.5935e-04 - val_loss: 3.9844e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.4803e-04 - val_loss: 3.9087e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.4275e-04 - val_loss: 3.9648e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.3924e-04 - val_loss: 3.9267e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.3781e-04 - val_loss: 3.9529e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.3772e-04 - val_loss: 4.0130e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.3683e-04 - val_loss: 3.9466e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.3735e-04 - val_loss: 3.9096e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.3871e-04 - val_loss: 3.9839e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.3755e-04 - val_loss: 3.8691e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.3837e-04 - val_loss: 4.0126e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.3828e-04 - val_loss: 3.9084e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.3704e-04 - val_loss: 3.8925e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.3622e-04 - val_loss: 3.8868e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.3611e-04 - val_loss: 3.9603e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.3102e-04 - val_loss: 3.7460e-04\n",
      "Epoch 18/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.3027e-04 - val_loss: 3.8241e-04\n",
      "Epoch 19/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.2787e-04 - val_loss: 3.7596e-04\n",
      "Epoch 20/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.2661e-04 - val_loss: 3.7772e-04\n",
      "Epoch 21/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.2731e-04 - val_loss: 3.7320e-04\n",
      "Epoch 22/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.2782e-04 - val_loss: 3.6962e-04\n",
      "Epoch 23/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.2806e-04 - val_loss: 3.7448e-04\n",
      "Epoch 24/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.2523e-04 - val_loss: 3.7480e-04\n",
      "Epoch 25/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.2460e-04 - val_loss: 3.6488e-04\n",
      "Epoch 26/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.2411e-04 - val_loss: 3.7149e-04\n",
      "Epoch 27/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 4.2685e-04 - val_loss: 3.7094e-04\n",
      "Epoch 28/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2651e-04 - val_loss: 3.6725e-04\n",
      "Epoch 29/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 4.2754e-04 - val_loss: 3.7832e-04\n",
      "Epoch 30/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2557e-04 - val_loss: 3.7526e-04\n",
      "Epoch 31/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2752e-04 - val_loss: 3.7642e-04\n",
      "Epoch 32/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 4.2592e-04 - val_loss: 3.6819e-04\n",
      "Epoch 33/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2541e-04 - val_loss: 3.6509e-04\n",
      "Epoch 34/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2406e-04 - val_loss: 3.6465e-04\n",
      "Epoch 35/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 4.2734e-04 - val_loss: 3.5757e-04\n",
      "Epoch 36/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 4.2322e-04 - val_loss: 3.7760e-04\n",
      "Epoch 37/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2496e-04 - val_loss: 3.7185e-04\n",
      "Epoch 38/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2691e-04 - val_loss: 3.7825e-04\n",
      "Epoch 39/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2519e-04 - val_loss: 3.6426e-04\n",
      "Epoch 40/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2516e-04 - val_loss: 3.6647e-04\n",
      "Epoch 41/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2380e-04 - val_loss: 3.6854e-04\n",
      "Epoch 42/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2367e-04 - val_loss: 3.7178e-04\n",
      "Epoch 43/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2361e-04 - val_loss: 3.7057e-04\n",
      "Epoch 44/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.2620e-04 - val_loss: 3.7449e-04\n",
      "Epoch 45/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2528e-04 - val_loss: 3.6276e-04\n",
      "Epoch 46/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2630e-04 - val_loss: 3.7798e-04\n",
      "Epoch 47/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.2439e-04 - val_loss: 3.6438e-04\n",
      "Epoch 47: early stopping\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp62s49h__/model/data/model/assets\n",
      "87428/87428 [==============================] - 252s 3ms/step - loss: 3.5437e-04\n",
      "9698/9698 [==============================] - 28s 3ms/step - loss: 3.6438e-04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 12:25:11 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/05 12:25:11 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 0.0018 - val_loss: 3.8899e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.3686e-04 - val_loss: 3.7097e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.2605e-04 - val_loss: 3.7263e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.2135e-04 - val_loss: 3.6568e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.2130e-04 - val_loss: 3.6797e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.1694e-04 - val_loss: 3.7422e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.1742e-04 - val_loss: 3.4692e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.2025e-04 - val_loss: 3.5470e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.1605e-04 - val_loss: 3.5729e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.1748e-04 - val_loss: 3.4862e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.1763e-04 - val_loss: 3.6334e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.1846e-04 - val_loss: 3.6618e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.1614e-04 - val_loss: 3.5769e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.1690e-04 - val_loss: 3.5484e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.1616e-04 - val_loss: 3.5613e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.1693e-04 - val_loss: 3.6317e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.1964e-04 - val_loss: 3.5975e-04\n",
      "Epoch 18/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.1827e-04 - val_loss: 3.6424e-04\n",
      "Epoch 19/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 4.1567e-04 - val_loss: 3.6541e-04\n",
      "Epoch 19: early stopping\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpteu00g9j/model/data/model/assets\n",
      "87428/87428 [==============================] - 244s 3ms/step - loss: 3.5110e-04\n",
      "9698/9698 [==============================] - 26s 3ms/step - loss: 3.6541e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 12:47:00 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/05 12:47:00 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 0.0010 - val_loss: 9.1663e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 9.0567e-04 - val_loss: 9.1897e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 47s 11ms/step - loss: 9.0566e-04 - val_loss: 9.0604e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 46s 11ms/step - loss: 9.0587e-04 - val_loss: 9.0784e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 46s 11ms/step - loss: 9.0557e-04 - val_loss: 9.1465e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 45s 10ms/step - loss: 9.0601e-04 - val_loss: 9.0885e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 46s 11ms/step - loss: 9.0579e-04 - val_loss: 9.0939e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 46s 11ms/step - loss: 9.0597e-04 - val_loss: 9.1797e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 45s 10ms/step - loss: 9.0565e-04 - val_loss: 9.1267e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 45s 10ms/step - loss: 9.0602e-04 - val_loss: 9.0464e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 46s 11ms/step - loss: 9.0557e-04 - val_loss: 9.1725e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 47s 11ms/step - loss: 9.0594e-04 - val_loss: 9.2176e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 9.0576e-04 - val_loss: 9.1055e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 45s 10ms/step - loss: 9.0560e-04 - val_loss: 9.0708e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 46s 10ms/step - loss: 9.0593e-04 - val_loss: 9.0814e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 45s 10ms/step - loss: 9.0570e-04 - val_loss: 9.0971e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 46s 10ms/step - loss: 9.0557e-04 - val_loss: 8.9360e-04\n",
      "Epoch 18/1000\n",
      "4372/4372 [==============================] - 45s 10ms/step - loss: 9.0570e-04 - val_loss: 9.0658e-04\n",
      "Epoch 19/1000\n",
      "4372/4372 [==============================] - 46s 10ms/step - loss: 9.0573e-04 - val_loss: 9.1745e-04\n",
      "Epoch 20/1000\n",
      "4372/4372 [==============================] - 46s 11ms/step - loss: 9.0560e-04 - val_loss: 9.1017e-04\n",
      "Epoch 21/1000\n",
      "4372/4372 [==============================] - 45s 10ms/step - loss: 9.0573e-04 - val_loss: 9.0641e-04\n",
      "Epoch 22/1000\n",
      "4372/4372 [==============================] - 46s 10ms/step - loss: 9.0593e-04 - val_loss: 8.9811e-04\n",
      "Epoch 23/1000\n",
      "4372/4372 [==============================] - 46s 10ms/step - loss: 9.0570e-04 - val_loss: 9.0988e-04\n",
      "Epoch 24/1000\n",
      "4372/4372 [==============================] - 45s 10ms/step - loss: 9.0562e-04 - val_loss: 9.1322e-04\n",
      "Epoch 25/1000\n",
      "4372/4372 [==============================] - 44s 10ms/step - loss: 9.0575e-04 - val_loss: 8.9947e-04\n",
      "Epoch 26/1000\n",
      "4372/4372 [==============================] - 45s 10ms/step - loss: 9.0585e-04 - val_loss: 9.1655e-04\n",
      "Epoch 27/1000\n",
      "4372/4372 [==============================] - 46s 11ms/step - loss: 9.0571e-04 - val_loss: 9.0354e-04\n",
      "Epoch 28/1000\n",
      "4372/4372 [==============================] - 47s 11ms/step - loss: 9.0570e-04 - val_loss: 8.9716e-04\n",
      "Epoch 29/1000\n",
      "4372/4372 [==============================] - 46s 10ms/step - loss: 9.0559e-04 - val_loss: 9.0956e-04\n",
      "Epoch 29: early stopping\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp5ice0_ng/model/data/model/assets\n",
      "87428/87428 [==============================] - 202s 2ms/step - loss: 9.0264e-04\n",
      "9698/9698 [==============================] - 23s 2ms/step - loss: 9.0956e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/05 13:13:39 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/05 13:13:39 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 9.8844e-04 - val_loss: 4.7939e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 5.4959e-04 - val_loss: 4.4082e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 5.1641e-04 - val_loss: 4.2650e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.7492e-04 - val_loss: 4.1409e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.6773e-04 - val_loss: 4.1319e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.7744e-04 - val_loss: 4.2230e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.8571e-04 - val_loss: 4.2850e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 5.0341e-04 - val_loss: 4.2775e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 50s 12ms/step - loss: 5.0270e-04 - val_loss: 4.5205e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 5.1275e-04 - val_loss: 4.6185e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 50s 12ms/step - loss: 5.1060e-04 - val_loss: 4.5671e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 5.0964e-04 - val_loss: 4.4816e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 5.0788e-04 - val_loss: 4.3652e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 5.0703e-04 - val_loss: 4.3891e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.9498e-04 - val_loss: 4.5270e-04\n",
      "Epoch 17: early stopping\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmplbomz7_1/model/data/model/assets\n",
      "58924/87428 [===================>..........] - ETA: 1:13 - loss: 4.4680e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 62s 14ms/step - loss: 4.1999e-04 - val_loss: 3.9833e-04\n",
      "Epoch 27/1000\n",
      "4372/4372 [==============================] - 63s 14ms/step - loss: 4.2113e-04 - val_loss: 3.8369e-04\n",
      "Epoch 28/1000\n",
      "4372/4372 [==============================] - 63s 14ms/step - loss: 4.2062e-04 - val_loss: 3.7231e-04\n",
      "Epoch 29/1000\n",
      "1697/4372 [==========>...................] - ETA: 35s - loss: 4.2009e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 48s 11ms/step - loss: 9.0129e-04 - val_loss: 9.1263e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 9.0162e-04 - val_loss: 8.8886e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 9.0149e-04 - val_loss: 9.0871e-04\n",
      "Epoch 18/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 9.0140e-04 - val_loss: 9.1591e-04\n",
      "Epoch 19/1000\n",
      "1027/4372 [======>.......................] - ETA: 33s - loss: 9.0156e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 54s 12ms/step - loss: 4.3854e-04 - val_loss: 3.8187e-04\n",
      "Epoch 29/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.3772e-04 - val_loss: 3.7623e-04\n",
      "Epoch 30/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 4.3786e-04 - val_loss: 3.7774e-04\n",
      "Epoch 31/1000\n",
      "2301/4372 [==============>...............] - ETA: 23s - loss: 4.3633e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78244/87428 [=========================>....] - ETA: 28s - loss: 3.4163e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 50s 11ms/step - loss: 2.9473e-04 - val_loss: 3.0136e-04\n",
      "Epoch 26/1000\n",
      "4372/4372 [==============================] - 50s 12ms/step - loss: 2.9476e-04 - val_loss: 2.9601e-04\n",
      "Epoch 27/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 2.9428e-04 - val_loss: 2.9524e-04\n",
      "Epoch 28/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 2.9498e-04 - val_loss: 2.9417e-04\n",
      "Epoch 29/1000\n",
      "1369/4372 [========>.....................] - ETA: 31s - loss: 2.9333e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.7365e-04 - val_loss: 2.7294e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 63s 14ms/step - loss: 2.7212e-04 - val_loss: 2.8172e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 2.6974e-04 - val_loss: 2.7733e-04\n",
      "Epoch 18/1000\n",
      "2270/4372 [==============>...............] - ETA: 27s - loss: 2.6839e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 52s 12ms/step - loss: 2.9206e-04 - val_loss: 2.9194e-04\n",
      "Epoch 31/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 2.9125e-04 - val_loss: 2.8761e-04\n",
      "Epoch 32/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 2.9126e-04 - val_loss: 2.9463e-04\n",
      "Epoch 33/1000\n",
      "3993/4372 [==========================>...] - ETA: 4s - loss: 2.9187e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 51s 12ms/step - loss: 3.2633e-04 - val_loss: 3.1369e-04\n",
      "Epoch 20/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 3.2524e-04 - val_loss: 3.0501e-04\n",
      "Epoch 21/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 3.2739e-04 - val_loss: 2.9668e-04\n",
      "Epoch 22/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 3.2583e-04 - val_loss: 3.1610e-04\n",
      "Epoch 23/1000\n",
      " 208/4372 [>.............................] - ETA: 44s - loss: 3.2825e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 63s 14ms/step - loss: 4.8189e-04 - val_loss: 4.4891e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 62s 14ms/step - loss: 4.7550e-04 - val_loss: 4.2352e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 63s 14ms/step - loss: 4.6437e-04 - val_loss: 4.0194e-04\n",
      "Epoch 6/1000\n",
      "3360/4372 [======================>.......] - ETA: 13s - loss: 4.5836e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.7367e-04 - val_loss: 3.5116e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 3.6720e-04 - val_loss: 3.6595e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.6159e-04 - val_loss: 3.3531e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 48s 11ms/step - loss: 3.5740e-04 - val_loss: 3.4891e-04\n",
      "Epoch 8/1000\n",
      "1824/4372 [===========>..................] - ETA: 26s - loss: 3.5415e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 50s 12ms/step - loss: 4.6757e-04 - val_loss: 3.8490e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.6814e-04 - val_loss: 3.9507e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.6776e-04 - val_loss: 3.9254e-04\n",
      "Epoch 12/1000\n",
      "2799/4372 [==================>...........] - ETA: 16s - loss: 4.6900e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9698/9698 [==============================] - 30s 3ms/step - loss: 8.9308e-04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/06 06:24:44 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/06 06:24:44 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 0.0013 - val_loss: 8.8179e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 8.7429e-04 - val_loss: 8.7832e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 8.7310e-04 - val_loss: 8.7370e-04\n",
      "Epoch 4/1000\n",
      "1463/4372 [=========>....................] - ETA: 29s - loss: 8.7332e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 50s 11ms/step - loss: 2.7543e-04 - val_loss: 2.7725e-04\n",
      "Epoch 24/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 2.7579e-04 - val_loss: 2.8217e-04\n",
      "Epoch 25/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 2.7563e-04 - val_loss: 2.8078e-04\n",
      "Epoch 26/1000\n",
      "2982/4372 [===================>..........] - ETA: 15s - loss: 2.7598e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.8734e-04 - val_loss: 2.9276e-04\n",
      "Epoch 36/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.8776e-04 - val_loss: 2.9576e-04\n",
      "Epoch 37/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.8976e-04 - val_loss: 2.9585e-04\n",
      "Epoch 38/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.8804e-04 - val_loss: 2.9320e-04\n",
      "Epoch 39/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.8682e-04 - val_loss: 2.8211e-04\n",
      "Epoch 40/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.8745e-04 - val_loss: 2.9209e-04\n",
      "Epoch 41/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.8832e-04 - val_loss: 2.9269e-04\n",
      "Epoch 42/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.8651e-04 - val_loss: 2.8692e-04\n",
      "Epoch 43/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.8867e-04 - val_loss: 2.9400e-04\n",
      "Epoch 44/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.8895e-04 - val_loss: 2.9517e-04\n",
      "Epoch 45/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.8760e-04 - val_loss: 2.8693e-04\n",
      "Epoch 46/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.8861e-04 - val_loss: 2.8089e-04\n",
      "Epoch 46: early stopping\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp56wd142h/model/data/model/assets\n",
      "87428/87428 [==============================] - 260s 3ms/step - loss: 2.6942e-04\n",
      "9698/9698 [==============================] - 30s 3ms/step - loss: 2.8089e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/06 09:32:27 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/06 09:32:27 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 0.0010 - val_loss: 5.5075e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 5.8754e-04 - val_loss: 4.8538e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 5.1442e-04 - val_loss: 4.3890e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 5.2832e-04 - val_loss: 4.9209e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 5.5316e-04 - val_loss: 5.3245e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 5.4511e-04 - val_loss: 5.4057e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 5.3534e-04 - val_loss: 5.5677e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 5.3066e-04 - val_loss: 5.5616e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 5.2819e-04 - val_loss: 5.5184e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 5.2687e-04 - val_loss: 5.5702e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 5.2487e-04 - val_loss: 5.5855e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 5.2408e-04 - val_loss: 5.6860e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 5.2337e-04 - val_loss: 5.7293e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 5.2251e-04 - val_loss: 5.7445e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 5.2190e-04 - val_loss: 5.8814e-04\n",
      "Epoch 15: early stopping\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp6nk8xd6h/model/data/model/assets\n",
      "87428/87428 [==============================] - 241s 3ms/step - loss: 5.8367e-04\n",
      "9698/9698 [==============================] - 27s 3ms/step - loss: 5.8814e-04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/06 09:50:54 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/06 09:50:54 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 55s 12ms/step - loss: 4.5180e-04 - val_loss: 3.2567e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.9918e-04 - val_loss: 2.9138e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.8043e-04 - val_loss: 2.8108e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.7293e-04 - val_loss: 2.8264e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.6754e-04 - val_loss: 2.8982e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.6412e-04 - val_loss: 2.8743e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.6138e-04 - val_loss: 2.8001e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.5950e-04 - val_loss: 2.7929e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.5701e-04 - val_loss: 2.7587e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.5544e-04 - val_loss: 2.7671e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.5403e-04 - val_loss: 2.7900e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 2.5286e-04 - val_loss: 2.8107e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.5195e-04 - val_loss: 2.8310e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.5237e-04 - val_loss: 2.8021e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 52s 12ms/step - loss: 2.5077e-04 - val_loss: 2.7351e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.4965e-04 - val_loss: 2.8600e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.4996e-04 - val_loss: 2.7692e-04\n",
      "Epoch 18/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.4825e-04 - val_loss: 2.7897e-04\n",
      "Epoch 19/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.4892e-04 - val_loss: 2.8671e-04\n",
      "Epoch 20/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.4812e-04 - val_loss: 2.7451e-04\n",
      "Epoch 21/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.4750e-04 - val_loss: 2.7976e-04\n",
      "Epoch 22/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.4716e-04 - val_loss: 2.8085e-04\n",
      "Epoch 23/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.4675e-04 - val_loss: 2.8759e-04\n",
      "Epoch 24/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.4561e-04 - val_loss: 2.8086e-04\n",
      "Epoch 25/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 2.4552e-04 - val_loss: 2.7874e-04\n",
      "Epoch 26/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.4450e-04 - val_loss: 2.7759e-04\n",
      "Epoch 27/1000\n",
      "4372/4372 [==============================] - 54s 12ms/step - loss: 2.4577e-04 - val_loss: 2.8159e-04\n",
      "Epoch 27: early stopping\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpcdub_2zy/model/data/model/assets\n",
      "87428/87428 [==============================] - 258s 3ms/step - loss: 2.3668e-04\n",
      "9698/9698 [==============================] - 28s 3ms/step - loss: 2.8159e-04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/06 10:20:25 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'dict'>. Dataset logging skipped.\n",
      "2024/09/06 10:20:25 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4372/4372 [==============================] - 53s 12ms/step - loss: 0.0028 - val_loss: 3.8746e-04\n",
      "Epoch 2/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.4103e-04 - val_loss: 3.7035e-04\n",
      "Epoch 3/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.2737e-04 - val_loss: 3.5707e-04\n",
      "Epoch 4/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.2016e-04 - val_loss: 3.5805e-04\n",
      "Epoch 5/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1625e-04 - val_loss: 3.5144e-04\n",
      "Epoch 6/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1511e-04 - val_loss: 3.5013e-04\n",
      "Epoch 7/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1419e-04 - val_loss: 3.4689e-04\n",
      "Epoch 8/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.1371e-04 - val_loss: 3.4227e-04\n",
      "Epoch 9/1000\n",
      "4372/4372 [==============================] - 50s 12ms/step - loss: 4.1328e-04 - val_loss: 3.5135e-04\n",
      "Epoch 10/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1338e-04 - val_loss: 3.5185e-04\n",
      "Epoch 11/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.1257e-04 - val_loss: 3.4298e-04\n",
      "Epoch 12/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.1139e-04 - val_loss: 3.3958e-04\n",
      "Epoch 13/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.1212e-04 - val_loss: 3.4283e-04\n",
      "Epoch 14/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.1294e-04 - val_loss: 3.4948e-04\n",
      "Epoch 15/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1240e-04 - val_loss: 3.4463e-04\n",
      "Epoch 16/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1156e-04 - val_loss: 3.3218e-04\n",
      "Epoch 17/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1174e-04 - val_loss: 3.4087e-04\n",
      "Epoch 18/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1061e-04 - val_loss: 3.3804e-04\n",
      "Epoch 19/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.1155e-04 - val_loss: 3.4393e-04\n",
      "Epoch 20/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.1149e-04 - val_loss: 3.4510e-04\n",
      "Epoch 21/1000\n",
      "4372/4372 [==============================] - 50s 12ms/step - loss: 4.1134e-04 - val_loss: 3.3859e-04\n",
      "Epoch 22/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.1150e-04 - val_loss: 3.3899e-04\n",
      "Epoch 23/1000\n",
      "4372/4372 [==============================] - 51s 12ms/step - loss: 4.1114e-04 - val_loss: 3.4325e-04\n",
      "Epoch 24/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1092e-04 - val_loss: 3.4926e-04\n",
      "Epoch 25/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.1079e-04 - val_loss: 3.4843e-04\n",
      "Epoch 26/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1054e-04 - val_loss: 3.3914e-04\n",
      "Epoch 27/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.1051e-04 - val_loss: 3.4279e-04\n",
      "Epoch 28/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1087e-04 - val_loss: 3.3013e-04\n",
      "Epoch 29/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.1109e-04 - val_loss: 3.5161e-04\n",
      "Epoch 30/1000\n",
      "4372/4372 [==============================] - 50s 11ms/step - loss: 4.1064e-04 - val_loss: 3.3847e-04\n",
      "Epoch 31/1000\n",
      "4372/4372 [==============================] - 50s 12ms/step - loss: 4.1019e-04 - val_loss: 3.4276e-04\n",
      "Epoch 32/1000\n",
      "4372/4372 [==============================] - 49s 11ms/step - loss: 4.1088e-04 - val_loss: 3.5286e-04\n",
      "Epoch 33/1000\n",
      "1312/4372 [========>.....................] - ETA: 31s - loss: 4.1118e-04"
     ]
    }
   ],
   "source": [
    "# AE\n",
    "\n",
    "for _ in range(100):\n",
    "    config = {'shape': 1957,\n",
    "              'output': 72,\n",
    "              'dropout': np.random.choice([0.01, 0.1, 0.2, 0.5]),\n",
    "              'num_layers': np.random.choice([3, 4, 5, 10]),\n",
    "              'num_neurons': np.random.choice([16, 32, 64, 128]),\n",
    "              'activation': np.random.choice(['gelu', 'gelu', 'relu'])}\n",
    "    \n",
    "    ae = np.random.choice([True, False, True, True])\n",
    "    if ae:\n",
    "        model = build_ae_model(config)\n",
    "    else:\n",
    "        model = build_model(config)\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        model_checkpoint_callback = ModelCheckpoint(\n",
    "            filepath=f\"models/model_{run.info.run_id}.keras\",\n",
    "            save_weights_only=False,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "        callback = EarlyStopping(monitor='val_loss', patience=12, verbose=1)\n",
    "        mlflow.log_param(\"Autoencoder\", ae)\n",
    "\n",
    "        history = model.fit(x_train, y_train, epochs=1000, batch_size=2000,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            callbacks=[callback, model_checkpoint_callback])\n",
    "        \n",
    "        evaled_test = model.evaluate(x_train, y_train, batch_size=100)\n",
    "        evaled_train = model.evaluate(x_test, y_test, batch_size=100)\n",
    "\n",
    "        mlflow.log_metric(\"test_loss\", evaled_test)\n",
    "        mlflow.log_metric(\"train_loss\", evaled_train)\n",
    "\n",
    "        plt.plot(history.history[\"loss\"])\n",
    "        plt.plot(history.history[\"val_loss\"])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plotname = f\"plots/loss_plot_{run.info.run_id}.png\"\n",
    "        plt.savefig(plotname)\n",
    "        plt.close()\n",
    "\n",
    "        # Log the plot as an artifact\n",
    "        mlflow.log_artifact(plotname)\n",
    "        #model_path = \"modles\"\n",
    "        #mlflow.tensorflow.log_model(tf_saved_model_dir=model_path, artifact_path=\"model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
